{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Higgs Boson - Collision event classification\n",
    "\n",
    "## Deep Learning Model - Retrain tuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload data to S3\n",
    "\n",
    "The data files have already been prepared locally on the storage of the Sagemaker Notebook instance. Next we'll upload them onto the default S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import accuracy_score, recall_score, roc_auc_score\n",
    "\n",
    "import os\n",
    "import time\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in remote mode.\n"
     ]
    }
   ],
   "source": [
    "# SageMaker session and role\n",
    "LOCAL_MODE = False\n",
    "if LOCAL_MODE:\n",
    "    # During model development it's more efficient to use SageMaker local model.\n",
    "    # https://aws.amazon.com/blogs/machine-learning/use-the-amazon-sagemaker-local-mode-to-train-on-your-notebook-instance/\n",
    "    sagemaker_session = sagemaker.LocalSession()\n",
    "\n",
    "    # If notebook instance does not have GPUs, change to 'local'.\n",
    "    train_instance_type = 'local'\n",
    "    deploy_instance_type = 'local'\n",
    "    print('Running in local mode.')\n",
    "else:\n",
    "    sagemaker_session = sagemaker.Session()\n",
    "    train_instance_type = 'ml.p2.xlarge'\n",
    "    deploy_instance_type = 'ml.m5.large'\n",
    "    print('Running in remote mode.')\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# Use the default Sagemaker an S3 bucket\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "# Prefix for objects in S3 bucket\n",
    "prefix = 'capstone/nn_1000000'\n",
    "\n",
    "# Location of data files on notebook storage.\n",
    "data_dir = '../data/1000000'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 locations: train s3://sagemaker-eu-west-1-019518462631/capstone/nn_1000000/train.csv / validation s3://sagemaker-eu-west-1-019518462631/capstone/nn_1000000/val.csv\n"
     ]
    }
   ],
   "source": [
    "# Actual upload of the data files.\n",
    "train_path = sagemaker_session.upload_data(data_dir+'/train.csv', bucket=bucket, key_prefix=prefix)\n",
    "val_path = sagemaker_session.upload_data(data_dir+'/val.csv', bucket=bucket, key_prefix=prefix)\n",
    "\n",
    "s3_input_train = sagemaker.s3_input(s3_data=train_path, content_type='csv')\n",
    "s3_input_val = sagemaker.s3_input(s3_data=val_path, content_type='csv')\n",
    "\n",
    "print(\"S3 locations: train {} / validation {}\".format(train_path, val_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 locations: test s3://sagemaker-eu-west-1-019518462631/capstone/nn_1000000/test-2.csv\n"
     ]
    }
   ],
   "source": [
    "# Read the test data.\n",
    "test_df = pd.read_csv(os.path.join(data_dir, 'test.csv'), header=None)\n",
    "\n",
    "# Seperate labels and features\n",
    "test_y = test_df.iloc[:,0]\n",
    "test_X = test_df.iloc[:, 1:]\n",
    "\n",
    "# Dump the test features and upload to S3.\n",
    "test_file = 'test-2.csv'\n",
    "pd.DataFrame(test_X).to_csv(os.path.join(data_dir, test_file), header=False, index=False)\n",
    "test_path = sagemaker_session.upload_data(os.path.join(data_dir, test_file), bucket=bucket, key_prefix=prefix)\n",
    "\n",
    "print(\"S3 locations: test {}\".format(test_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ../data/1000000/scalers.pkl to s3://sagemaker-eu-west-1-019518462631/capstone/nn_1000000/scalers.pkl\n",
      "S3 locations: scalers s3://sagemaker-eu-west-1-019518462631/capstone/nn_1000000/scalers.pkl\n"
     ]
    }
   ],
   "source": [
    "# Upload sklearn scalers model.\n",
    "scalers_path = \"s3://{}/{}/scalers.pkl\".format(bucket, prefix)\n",
    "!aws s3 cp $data_dir/scalers.pkl $scalers_path\n",
    "\n",
    "print(\"S3 locations: scalers {}\".format(scalers_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "capstone/nn_1000000/test-2.csv\n",
      "capstone/nn_1000000/train.csv\n",
      "capstone/nn_1000000/val.csv\n"
     ]
    }
   ],
   "source": [
    "# Check if files have been uploaded\n",
    "for obj in boto3.resource('s3').Bucket(bucket).objects.all():\n",
    "    if prefix in obj.key and '.csv' in obj.key:\n",
    "        print(obj.key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep learning model\n",
    "\n",
    "Next I wanted to evaluate the performance of a deep neural network.\n",
    "\n",
    "After several try-outs I'm using following model.\n",
    "\n",
    "* Input layer\n",
    "    * Fully connected layer\n",
    "    * Tanh activation function\n",
    "    * Batch normalization\n",
    "    * Drop out\n",
    "\n",
    "* 3 hidden layers\n",
    "    * Fully connected layer\n",
    "    * Tanh activation function\n",
    "    * Batch normalization\n",
    "    * Drop out\n",
    "\n",
    "* Output layer\n",
    "    * Fully connected layer (with 1 output node)\n",
    "    * Sigmoid activation function\n",
    "    \n",
    "The cells below show the code of the model and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnn\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mfunctional\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mF\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mNet\u001b[39;49;00m(nn.Module):\n",
      "    \n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, input_dim, hidden_dim, init_weights=\u001b[33m'\u001b[39;49;00m\u001b[33mnone\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, dropout_rate=\u001b[34m0.5\u001b[39;49;00m):\n",
      "        \u001b[33m'''Defines layers of a neural network.\u001b[39;49;00m\n",
      "\u001b[33m           :param input_dim: Number of input features\u001b[39;49;00m\n",
      "\u001b[33m           :param hidden\u001b[39;49;00m\n",
      "\u001b[33m           :param hidden_dim: Size of hidden layer(s)\u001b[39;49;00m\n",
      "\u001b[33m           :param output_dim: Number of outputs\u001b[39;49;00m\n",
      "\u001b[33m         '''\u001b[39;49;00m\n",
      "        \u001b[36msuper\u001b[39;49;00m(Net, \u001b[36mself\u001b[39;49;00m).\u001b[32m__init__\u001b[39;49;00m()\n",
      "        \n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mMaking Net with input \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m, hidden \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(input_dim, hidden_dim))\n",
      "\n",
      "        \u001b[36mself\u001b[39;49;00m.model = nn.Sequential(\n",
      "            \n",
      "            \u001b[37m# Input layer\u001b[39;49;00m\n",
      "            nn.Linear(input_dim, hidden_dim),\n",
      "            nn.Tanh(),\n",
      "            nn.BatchNorm1d(hidden_dim),\n",
      "            nn.Dropout(dropout_rate),\n",
      "\n",
      "            \u001b[37m# Hidden layers\u001b[39;49;00m\n",
      "            nn.Linear(hidden_dim, hidden_dim),\n",
      "            nn.Tanh(),\n",
      "            nn.BatchNorm1d(hidden_dim),\n",
      "            nn.Dropout(dropout_rate),\n",
      "\n",
      "            nn.Linear(hidden_dim, hidden_dim),\n",
      "            nn.Tanh(),\n",
      "            nn.BatchNorm1d(hidden_dim),\n",
      "            nn.Dropout(dropout_rate),\n",
      "\n",
      "            nn.Linear(hidden_dim, hidden_dim),\n",
      "            nn.Tanh(),\n",
      "            nn.BatchNorm1d(hidden_dim),\n",
      "            nn.Dropout(dropout_rate),\n",
      "                       \n",
      "            \u001b[37m# Output layer\u001b[39;49;00m\n",
      "            nn.Linear(hidden_dim, \u001b[34m1\u001b[39;49;00m),\n",
      "        )\n",
      "        \u001b[36mself\u001b[39;49;00m.sig = nn.Sigmoid()\n",
      "\n",
      "        \u001b[34mif\u001b[39;49;00m init_weights == \u001b[33m'\u001b[39;49;00m\u001b[33mxavier_uniform\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
      "            \u001b[36mself\u001b[39;49;00m.model.apply(Net.init_weights_xavier_uniform)\n",
      "        \u001b[34melif\u001b[39;49;00m init_weights == \u001b[33m'\u001b[39;49;00m\u001b[33mxavier_normal\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
      "            \u001b[36mself\u001b[39;49;00m.model.apply(Net.init_weights_xavier_normal)\n",
      "        \u001b[34melif\u001b[39;49;00m init_weights == \u001b[33m'\u001b[39;49;00m\u001b[33muniform\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
      "            \u001b[36mself\u001b[39;49;00m.model.apply(Net.init_weights_uniform)\n",
      "\n",
      "    \u001b[90m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32minit_weights_xavier_uniform\u001b[39;49;00m(model):\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[36mtype\u001b[39;49;00m(model) == nn.Linear:\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mSetting initial weights on \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(\u001b[36mstr\u001b[39;49;00m(model)))\n",
      "            torch.nn.init.xavier_uniform_(model.weight)\n",
      "            \u001b[34mif\u001b[39;49;00m model.bias \u001b[35mis\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m:\n",
      "                model.bias.data.fill_(\u001b[34m0\u001b[39;49;00m)    \n",
      "\n",
      "    \u001b[90m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32minit_weights_xavier_normal\u001b[39;49;00m(model):\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[36mtype\u001b[39;49;00m(model) == nn.Linear:\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mSetting initial weights on \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(\u001b[36mstr\u001b[39;49;00m(model)))\n",
      "            torch.nn.init.xavier_normal_(model.weight)\n",
      "            \u001b[34mif\u001b[39;49;00m model.bias \u001b[35mis\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m:\n",
      "                model.bias.data.fill_(\u001b[34m0\u001b[39;49;00m)    \n",
      "\n",
      "    \u001b[90m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32minit_weights_uniform\u001b[39;49;00m(model):\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[36mtype\u001b[39;49;00m(model) == nn.Linear:\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mSetting initial weights on \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(\u001b[36mstr\u001b[39;49;00m(model)))\n",
      "            n = model.in_features\n",
      "            y = \u001b[34m1.0\u001b[39;49;00m/np.sqrt(n)\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mY=\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(y))\n",
      "            model.weight.data.uniform_(-y, y)\n",
      "            \u001b[34mif\u001b[39;49;00m model.bias \u001b[35mis\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m:\n",
      "                model.bias.data.fill_(\u001b[34m0\u001b[39;49;00m)    \n",
      "\n",
      "\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mforward\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, x):\n",
      "        \u001b[33m'''Feedforward behavior of the net.\u001b[39;49;00m\n",
      "\u001b[33m           :param x: A batch of input features\u001b[39;49;00m\n",
      "\u001b[33m           :return: A single, sigmoid activated value\u001b[39;49;00m\n",
      "\u001b[33m         '''\u001b[39;49;00m\n",
      "\n",
      "        x = \u001b[36mself\u001b[39;49;00m.model(x)\n",
      "        \u001b[34mreturn\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.sig(x)\n"
     ]
    }
   ],
   "source": [
    "!pygmentize source/model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36m__future__\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m print_function \u001b[37m# future proof\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mboto3\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mre\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjoblib\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# pytorch\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnn\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36moptim\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36moptim\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdata\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# import model\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mmodel\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Net\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_fn\u001b[39;49;00m(model_dir):\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mLoading model.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[34mglobal\u001b[39;49;00m scalers\n",
      "\n",
      "    \u001b[37m# First, load the parameters used to create the model.\u001b[39;49;00m\n",
      "    model_info = {}\n",
      "    model_info_path = os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel_info.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(model_info_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
      "        model_info = torch.load(f)\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mmodel_info: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(model_info))\n",
      "\n",
      "    \u001b[37m# Determine the device and construct the model.\u001b[39;49;00m\n",
      "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    model = Net(model_info[\u001b[33m'\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], \n",
      "                model_info[\u001b[33m'\u001b[39;49;00m\u001b[33mhidden_dim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    \n",
      "    scalers = model_info[\u001b[33m'\u001b[39;49;00m\u001b[33mscalers\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "\n",
      "    \u001b[37m# Load the stored model parameters.\u001b[39;49;00m\n",
      "    model_path = os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(model_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
      "        model.load_state_dict(torch.load(f))\n",
      "    \n",
      "    \u001b[34mreturn\u001b[39;49;00m model.to(device)\n",
      "\n",
      "\n",
      "\u001b[37m# Load the data from a csv file\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_get_data_loader\u001b[39;49;00m(batch_size, data_dir, data_file):\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mGet data loader for file \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(data_file))\n",
      "\n",
      "    data = pd.read_csv(os.path.join(data_dir, data_file), header=\u001b[34mNone\u001b[39;49;00m, names=\u001b[34mNone\u001b[39;49;00m)\n",
      "    y = torch.from_numpy(data[[\u001b[34m0\u001b[39;49;00m]].values).float().squeeze()\n",
      "    x = torch.from_numpy(data.drop([\u001b[34m0\u001b[39;49;00m], axis=\u001b[34m1\u001b[39;49;00m).values).float()\n",
      "    ds = torch.utils.data.TensorDataset(x, y)\n",
      "    \u001b[34mreturn\u001b[39;49;00m torch.utils.data.DataLoader(ds, batch_size=batch_size, shuffle=\u001b[34mTrue\u001b[39;49;00m)\n",
      "\n",
      "    \n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtrain_epoch\u001b[39;49;00m(model, device, train_loader, optimizer, criterion, epoch):\n",
      "    \u001b[33m\"\"\" Perform training for a single epoch.\u001b[39;49;00m\n",
      "\u001b[33m    \u001b[39;49;00m\n",
      "\u001b[33m    :param model: PyTorch model to train\u001b[39;49;00m\n",
      "\u001b[33m    :param device: Where the model and data should be loaded (cpu or gpu)\u001b[39;49;00m\n",
      "\u001b[33m    :param train_loader: PyTorch DataLoader used during training\u001b[39;49;00m\n",
      "\u001b[33m    :param optimizer: The optimizer used during training\u001b[39;49;00m\n",
      "\u001b[33m    :param criterion: The loss function used for training\u001b[39;49;00m\n",
      "\u001b[33m    :param epoch: Epoch that the model is being trained for\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "    model.train()\n",
      "    model.to(device)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m  Train: model on \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(device))\n",
      "    \n",
      "    \n",
      "    total_loss = \u001b[34m0\u001b[39;49;00m\n",
      "\n",
      "    \u001b[34mfor\u001b[39;49;00m batch_idx, (batch_X, batch_y) \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(train_loader, \u001b[34m1\u001b[39;49;00m):\n",
      "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
      "\n",
      "        optimizer.zero_grad()\n",
      "        output = model(batch_X).view(-\u001b[34m1\u001b[39;49;00m)\n",
      "        loss = criterion(output, batch_y)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        total_loss += loss.data.item()\n",
      "        \n",
      "    avg_loss = total_loss/\u001b[36mlen\u001b[39;49;00m(train_loader)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m  Train: Average loss: \u001b[39;49;00m\u001b[33m{:.4f}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(avg_loss))\n",
      "    \n",
      "    \u001b[34mreturn\u001b[39;49;00m avg_loss\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtest_model\u001b[39;49;00m(model, device, test_loader, criterion):\n",
      "    model.eval()\n",
      "    model.to(device)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m  Validation: model on \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(device))\n",
      "\n",
      "    test_losses = []\n",
      "    total_loss = \u001b[34m0\u001b[39;49;00m\n",
      "\n",
      "    \u001b[34mwith\u001b[39;49;00m torch.no_grad():\n",
      "        \u001b[34mfor\u001b[39;49;00m batch_idx, (batch_X, batch_y) \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(test_loader, \u001b[34m1\u001b[39;49;00m):\n",
      "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
      "            output = model(batch_X).view(-\u001b[34m1\u001b[39;49;00m)\n",
      "\n",
      "            test_loss = criterion(output, batch_y)\n",
      "            total_loss += test_loss.data.item()\n",
      "\n",
      "    avg_test_loss = total_loss / \u001b[36mlen\u001b[39;49;00m(test_loader)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m  Validation: Average loss: \u001b[39;49;00m\u001b[33m{:.4f}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(avg_test_loss))\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m avg_test_loss\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtrain_model\u001b[39;49;00m(model, device, train_loader, test_loader, optimizer, criterion, epochs, model_dir):\n",
      "    best_val_loss = \u001b[34mNone\u001b[39;49;00m\n",
      "    best_val_loss_epoch = -\u001b[34m1\u001b[39;49;00m\n",
      "    best_train_loss = \u001b[34mNone\u001b[39;49;00m\n",
      "    best_train_loss_epoch = -\u001b[34m1\u001b[39;49;00m\n",
      "    \u001b[34mfor\u001b[39;49;00m epoch \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(\u001b[34m1\u001b[39;49;00m, epochs + \u001b[34m1\u001b[39;49;00m):\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mEpoch \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(epoch))\n",
      "        train_loss = train_epoch(model, device, train_loader, optimizer, criterion, epoch)\n",
      "        val_loss = test_model(model, device, test_loader, criterion)\n",
      "\n",
      "        \u001b[34mif\u001b[39;49;00m best_train_loss \u001b[35mis\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m \u001b[35mor\u001b[39;49;00m train_loss < best_train_loss:\n",
      "            best_train_loss = train_loss\n",
      "            best_train_loss_epoch = epoch\n",
      "        \u001b[34mif\u001b[39;49;00m best_val_loss \u001b[35mis\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m \u001b[35mor\u001b[39;49;00m val_loss < best_val_loss:\n",
      "            best_val_loss = val_loss\n",
      "            best_val_loss_epoch = epoch\n",
      "            save_model(model, model_dir)\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m  Model performed better on validation...Saved.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "            \n",
      "        \u001b[34mif\u001b[39;49;00m val_loss > best_val_loss \u001b[35mand\u001b[39;49;00m epoch - best_val_loss_epoch >= \u001b[34m10\u001b[39;49;00m:\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m  Validation loss didn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mt improve over last 10 epochs. Stopping\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "            \u001b[34mbreak\u001b[39;49;00m\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mBest training loss: \u001b[39;49;00m\u001b[33m{:.4f}\u001b[39;49;00m\u001b[33m in epoch \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(best_train_loss, best_train_loss_epoch))\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mBest validation loss: \u001b[39;49;00m\u001b[33m{:.4f}\u001b[39;49;00m\u001b[33m in epoch \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(best_val_loss, best_val_loss_epoch))\n",
      "\n",
      "\n",
      "\u001b[37m# Provided model saving functions\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32msave_model\u001b[39;49;00m(model, model_dir):\n",
      "    path = os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# save state dictionary\u001b[39;49;00m\n",
      "    torch.save(model.cpu().state_dict(), path)\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32msave_model_params\u001b[39;49;00m(model_info, model_dir):\n",
      "    model_info_path = os.path.join(args.model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel_info.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mSaving model params in \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(model_info_path))\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(model_info_path, \u001b[33m'\u001b[39;49;00m\u001b[33mwb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
      "        torch.save(model_info, f)\n",
      "\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m'\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
      "    \u001b[37m# All of the model parameters and training parameters are sent as arguments\u001b[39;49;00m\n",
      "    \u001b[37m# when this script is executed, during a training job\u001b[39;49;00m\n",
      "    \n",
      "    \u001b[37m# Here we set up an argument parser to easily access the parameters\u001b[39;49;00m\n",
      "    parser = argparse.ArgumentParser()\n",
      "\n",
      "    \u001b[37m# SageMaker parameters, like the directories for training data and saving models; set automatically\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--hosts\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mlist\u001b[39;49;00m, default=json.loads(os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_HOSTS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]))\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--current-host\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CURRENT_HOST\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--model-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--train-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAIN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--validation-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_VALIDATION\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--scalers-config\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[37m# Training Parameters\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--batch-size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m64\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33minput batch size for training (default: 64)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m50\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mnumber of epochs to train (default: 10)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--optimizer\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m'\u001b[39;49;00m\u001b[33madam\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mOPTIM\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33moptimizer (default: adam)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--lr\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.001\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mLR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mlearning rate (default: 0.001)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--momentum\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mM\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mlearning rate (default: 0.001)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--seed\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m42\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mrandom seed (default: 42)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--weigth-init\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m'\u001b[39;49;00m\u001b[33mnone\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mweight init method (default: pytorch default)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "  \n",
      "    \u001b[37m# Model parameters\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--input-dim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m2\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mIN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33minput batch size for training (default: 2)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--hidden-dim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m50\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mH\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33minput batch size for training (default: 50)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--dropout-rate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.2\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mDR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mdropout rate (default: 0.2)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \n",
      "    args = parser.parse_args()\n",
      "\n",
      "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mModel will be trained on \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(device))\n",
      "    \n",
      "    \u001b[37m# set the seed for generating random numbers\u001b[39;49;00m\n",
      "    torch.manual_seed(args.seed)\n",
      "    \u001b[34mif\u001b[39;49;00m torch.cuda.is_available():\n",
      "        torch.cuda.manual_seed(args.seed)\n",
      "        \n",
      "    m = re.match(\u001b[33m'\u001b[39;49;00m\u001b[33ms3://([^/]+)/(.+)$\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, args.scalers_config)\n",
      "    \u001b[34mif\u001b[39;49;00m m \u001b[35mis\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m:\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mScaler config param invalid (\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m).\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(args.scalers_config))\n",
      "        sys.exit(\u001b[34m1\u001b[39;49;00m)\n",
      "\n",
      "    bucket = m.group(\u001b[34m1\u001b[39;49;00m)\n",
      "    key = m.group(\u001b[34m2\u001b[39;49;00m)\n",
      "\n",
      "    boto3.resource(\u001b[33m'\u001b[39;49;00m\u001b[33ms3\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).Bucket(bucket).download_file(key, \u001b[33m'\u001b[39;49;00m\u001b[33mscaler.pkl\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mscaler.pkl\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
      "        scalers = joblib.load(f)\n",
      "        \n",
      "    \u001b[37m# get train loader\u001b[39;49;00m\n",
      "    train_loader = _get_data_loader(args.batch_size, args.train_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mtrain.csv\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    test_loader = _get_data_loader(args.batch_size, args.validation_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mval.csv\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \n",
      "    model = Net(args.input_dim, args.hidden_dim, init_weights=args.weigth_init, dropout_rate=args.dropout_rate).to(device)\n",
      "    \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mShow model:\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[36mprint\u001b[39;49;00m(model)\n",
      "\n",
      "    model_info = {\n",
      "        \u001b[33m'\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: args.input_dim,\n",
      "        \u001b[33m'\u001b[39;49;00m\u001b[33mhidden_dim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: args.hidden_dim,\n",
      "        \u001b[33m'\u001b[39;49;00m\u001b[33mdropout_rate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: args.dropout_rate,\n",
      "        \u001b[33m'\u001b[39;49;00m\u001b[33mscalers\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: scalers\n",
      "    }\n",
      "    save_model_params(model_info, args.model_dir)\n",
      "\n",
      "    optimzers = {\n",
      "        \u001b[33m'\u001b[39;49;00m\u001b[33madam\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: optim.Adam(model.parameters(), lr=args.lr),\n",
      "        \u001b[33m'\u001b[39;49;00m\u001b[33msgd\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n",
      "    }\n",
      "    optimizer = optimzers[args.optimizer]\n",
      "    criterion = nn.BCELoss()\n",
      "        \n",
      "    train_model(model, device, train_loader, test_loader, optimizer, criterion, args.epochs, args.model_dir)\n"
     ]
    }
   ],
   "source": [
    "!pygmentize source/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyTorch estimator\n",
    "\n",
    "Let's retrain a PyTorch classifier on a data set of 1000000 items using the best hyperparameters.\n",
    "\n",
    "* Batch size: 300\n",
    "* Size of hidden layers: 680\n",
    "* Epochs: 100 (if validation loss does not reduce for 10 epochs, training is stopped).\n",
    "* Dropout rate: 0.3\n",
    "* Learning rate: 0.001\n",
    "* Optimizer: ADAM\n",
    "* Weight initialization: Uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 locations: output path s3://sagemaker-eu-west-1-019518462631/capstone/nn_1000000\n"
     ]
    }
   ],
   "source": [
    "# import a PyTorch wrapper\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "# specify an output path\n",
    "pyt_output_path = \"s3://{}/{}\".format(bucket, prefix)\n",
    "print(\"S3 locations: output path {}\".format(pyt_output_path))\n",
    "\n",
    "scalers_config_location = '{}/scalers.pkl'.format(pyt_output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n",
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-21 08:33:56 Starting - Starting the training job...\n",
      "2020-07-21 08:33:57 Starting - Launching requested ML instances......\n",
      "2020-07-21 08:35:13 Starting - Preparing the instances for training.........\n",
      "2020-07-21 08:36:48 Downloading - Downloading input data......\n",
      "2020-07-21 08:37:38 Training - Downloading the training image.........\n",
      "2020-07-21 08:39:13 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2020-07-21 08:39:15,093 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2020-07-21 08:39:15,118 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2020-07-21 08:39:16,558 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-07-21 08:39:16,929 sagemaker-containers INFO     Module default_user_module_name does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2020-07-21 08:39:16,929 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2020-07-21 08:39:16,930 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2020-07-21 08:39:16,930 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python -m pip install . -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing /tmp/tmppq5tguom/module_dir\u001b[0m\n",
      "\u001b[34mCollecting scikit-learn==0.22.1\n",
      "  Downloading scikit_learn-0.22.1-cp36-cp36m-manylinux1_x86_64.whl (7.0 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.11.0 in /opt/conda/lib/python3.6/site-packages (from scikit-learn==0.22.1->-r requirements.txt (line 1)) (1.16.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.6/site-packages (from scikit-learn==0.22.1->-r requirements.txt (line 1)) (0.15.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy>=0.17.0 in /opt/conda/lib/python3.6/site-packages (from scikit-learn==0.22.1->-r requirements.txt (line 1)) (1.2.2)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: default-user-module-name\n",
      "  Building wheel for default-user-module-name (setup.py): started\n",
      "  Building wheel for default-user-module-name (setup.py): finished with status 'done'\n",
      "  Created wheel for default-user-module-name: filename=default_user_module_name-1.0.0-py2.py3-none-any.whl size=22628 sha256=75a89bf72a41e00e181824c6ffd9089f9cb43b90323edd06811633e80ed4082b\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-05dr86fp/wheels/a9/47/06/cd2e9ebfcbec788dcf972d851875d24f72a26c1cb8b792b980\u001b[0m\n",
      "\u001b[34mSuccessfully built default-user-module-name\u001b[0m\n",
      "\u001b[34mInstalling collected packages: scikit-learn, default-user-module-name\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.21.2\n",
      "    Uninstalling scikit-learn-0.21.2:\n",
      "      Successfully uninstalled scikit-learn-0.21.2\u001b[0m\n",
      "\u001b[34mSuccessfully installed default-user-module-name-1.0.0 scikit-learn-0.22.1\u001b[0m\n",
      "\u001b[34m2020-07-21 08:39:22,479 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"validation\": \"/opt/ml/input/data/validation\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"lr\": 0.001,\n",
      "        \"input-dim\": 17,\n",
      "        \"hidden-dim\": 680,\n",
      "        \"scalers-config\": \"s3://sagemaker-eu-west-1-019518462631/capstone/nn_1000000/scalers.pkl\",\n",
      "        \"dropout-rate\": 0.3,\n",
      "        \"batch-size\": 300,\n",
      "        \"optimizer\": \"adam\",\n",
      "        \"weigth-init\": \"uniform\",\n",
      "        \"epochs\": 100\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"validation\": {\n",
      "            \"ContentType\": \"csv\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"ContentType\": \"csv\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2020-07-21-08-33-55-977\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-eu-west-1-019518462631/pytorch-training-2020-07-21-08-33-55-977/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch-size\":300,\"dropout-rate\":0.3,\"epochs\":100,\"hidden-dim\":680,\"input-dim\":17,\"lr\":0.001,\"optimizer\":\"adam\",\"scalers-config\":\"s3://sagemaker-eu-west-1-019518462631/capstone/nn_1000000/scalers.pkl\",\"weigth-init\":\"uniform\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"ContentType\":\"csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"ContentType\":\"csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\",\"validation\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-eu-west-1-019518462631/pytorch-training-2020-07-21-08-33-55-977/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\",\"validation\":\"/opt/ml/input/data/validation\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch-size\":300,\"dropout-rate\":0.3,\"epochs\":100,\"hidden-dim\":680,\"input-dim\":17,\"lr\":0.001,\"optimizer\":\"adam\",\"scalers-config\":\"s3://sagemaker-eu-west-1-019518462631/capstone/nn_1000000/scalers.pkl\",\"weigth-init\":\"uniform\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"ContentType\":\"csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"ContentType\":\"csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2020-07-21-08-33-55-977\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-eu-west-1-019518462631/pytorch-training-2020-07-21-08-33-55-977/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch-size\",\"300\",\"--dropout-rate\",\"0.3\",\"--epochs\",\"100\",\"--hidden-dim\",\"680\",\"--input-dim\",\"17\",\"--lr\",\"0.001\",\"--optimizer\",\"adam\",\"--scalers-config\",\"s3://sagemaker-eu-west-1-019518462631/capstone/nn_1000000/scalers.pkl\",\"--weigth-init\",\"uniform\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VALIDATION=/opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_LR=0.001\u001b[0m\n",
      "\u001b[34mSM_HP_INPUT-DIM=17\u001b[0m\n",
      "\u001b[34mSM_HP_HIDDEN-DIM=680\u001b[0m\n",
      "\u001b[34mSM_HP_SCALERS-CONFIG=s3://sagemaker-eu-west-1-019518462631/capstone/nn_1000000/scalers.pkl\u001b[0m\n",
      "\u001b[34mSM_HP_DROPOUT-RATE=0.3\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH-SIZE=300\u001b[0m\n",
      "\u001b[34mSM_HP_OPTIMIZER=adam\u001b[0m\n",
      "\u001b[34mSM_HP_WEIGTH-INIT=uniform\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=100\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python train.py --batch-size 300 --dropout-rate 0.3 --epochs 100 --hidden-dim 680 --input-dim 17 --lr 0.001 --optimizer adam --scalers-config s3://sagemaker-eu-west-1-019518462631/capstone/nn_1000000/scalers.pkl --weigth-init uniform\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mModel will be trained on cuda\u001b[0m\n",
      "\u001b[34mGet data loader for file train.csv.\u001b[0m\n",
      "\u001b[34mGet data loader for file val.csv.\u001b[0m\n",
      "\u001b[34mMaking Net with input 17, hidden 680\u001b[0m\n",
      "\u001b[34mSetting initial weights on Linear(in_features=17, out_features=680, bias=True)\u001b[0m\n",
      "\u001b[34mY=0.24253562503633297\u001b[0m\n",
      "\u001b[34mSetting initial weights on Linear(in_features=680, out_features=680, bias=True)\u001b[0m\n",
      "\u001b[34mY=0.03834824944236852\u001b[0m\n",
      "\u001b[34mSetting initial weights on Linear(in_features=680, out_features=680, bias=True)\u001b[0m\n",
      "\u001b[34mY=0.03834824944236852\u001b[0m\n",
      "\u001b[34mSetting initial weights on Linear(in_features=680, out_features=680, bias=True)\u001b[0m\n",
      "\u001b[34mY=0.03834824944236852\u001b[0m\n",
      "\u001b[34mSetting initial weights on Linear(in_features=680, out_features=1, bias=True)\u001b[0m\n",
      "\u001b[34mY=0.03834824944236852\u001b[0m\n",
      "\u001b[34mShow model:\u001b[0m\n",
      "\u001b[34mNet(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=17, out_features=680, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): BatchNorm1d(680, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Dropout(p=0.3, inplace=False)\n",
      "    (4): Linear(in_features=680, out_features=680, bias=True)\n",
      "    (5): Tanh()\n",
      "    (6): BatchNorm1d(680, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): Dropout(p=0.3, inplace=False)\n",
      "    (8): Linear(in_features=680, out_features=680, bias=True)\n",
      "    (9): Tanh()\n",
      "    (10): BatchNorm1d(680, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): Dropout(p=0.3, inplace=False)\n",
      "    (12): Linear(in_features=680, out_features=680, bias=True)\n",
      "    (13): Tanh()\n",
      "    (14): BatchNorm1d(680, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (15): Dropout(p=0.3, inplace=False)\n",
      "    (16): Linear(in_features=680, out_features=1, bias=True)\n",
      "  )\n",
      "  (sig): Sigmoid()\u001b[0m\n",
      "\u001b[34m)\u001b[0m\n",
      "\u001b[34mSaving model params in /opt/ml/model/model_info.pth\u001b[0m\n",
      "\u001b[34mEpoch 1\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m[2020-07-21 08:39:32.473 algo-1:43 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2020-07-21 08:39:32.473 algo-1:43 INFO hook.py:191] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2020-07-21 08:39:32.473 algo-1:43 INFO hook.py:236] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2020-07-21 08:39:32.473 algo-1:43 INFO state_store.py:67] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2020-07-21 08:39:32.488 algo-1:43 INFO hook.py:376] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2020-07-21 08:39:32.488 algo-1:43 INFO hook.py:437] Hook is writing from the hook with pid: 43\n",
      "\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6493\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6176\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 2\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6242\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6074\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 3\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6164\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5997\u001b[0m\n",
      "\u001b[34m  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 4\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6117\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5976\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 5\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6086\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5935\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 6\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6064\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5906\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 7\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6047\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5898\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 8\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6026\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5894\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 9\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6014\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5886\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 10\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6002\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5865\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 11\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5995\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5862\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 12\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5990\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5861\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 13\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5979\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5849\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 14\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5970\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5851\u001b[0m\n",
      "\u001b[34mEpoch 15\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5967\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5845\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 16\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5959\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5834\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 17\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5954\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5829\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 18\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5949\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5828\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 19\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5937\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5830\u001b[0m\n",
      "\u001b[34mEpoch 20\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5934\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5814\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 21\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5929\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5811\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 22\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5926\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5811\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 23\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5922\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5814\u001b[0m\n",
      "\u001b[34mEpoch 24\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5922\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5815\u001b[0m\n",
      "\u001b[34mEpoch 25\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5918\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5810\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 26\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5916\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5807\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 27\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5910\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5795\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 28\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5910\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5802\u001b[0m\n",
      "\u001b[34mEpoch 29\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5903\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5798\u001b[0m\n",
      "\u001b[34mEpoch 30\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5904\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5801\u001b[0m\n",
      "\u001b[34mEpoch 31\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5904\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5789\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 32\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5901\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5786\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 33\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5898\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5785\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 34\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5896\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5791\u001b[0m\n",
      "\u001b[34mEpoch 35\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5893\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5786\u001b[0m\n",
      "\u001b[34mEpoch 36\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5892\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5790\u001b[0m\n",
      "\u001b[34mEpoch 37\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5890\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5779\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 38\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5888\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5782\u001b[0m\n",
      "\u001b[34mEpoch 39\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5883\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5772\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 40\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5880\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5772\u001b[0m\n",
      "\u001b[34mEpoch 41\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5882\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5767\u001b[0m\n",
      "\u001b[34m  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 42\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5873\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5768\u001b[0m\n",
      "\u001b[34mEpoch 43\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5877\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5758\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 44\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5872\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5763\u001b[0m\n",
      "\u001b[34mEpoch 45\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5865\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5776\u001b[0m\n",
      "\u001b[34mEpoch 46\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5870\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5760\u001b[0m\n",
      "\u001b[34mEpoch 47\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5869\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5763\u001b[0m\n",
      "\u001b[34mEpoch 48\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5867\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5761\u001b[0m\n",
      "\u001b[34mEpoch 49\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5869\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5766\u001b[0m\n",
      "\u001b[34mEpoch 50\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5864\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5755\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 51\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5863\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5757\u001b[0m\n",
      "\u001b[34mEpoch 52\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5862\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5778\u001b[0m\n",
      "\u001b[34mEpoch 53\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5865\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5757\u001b[0m\n",
      "\u001b[34mEpoch 54\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5861\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5747\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 55\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5859\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5755\u001b[0m\n",
      "\u001b[34mEpoch 56\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5855\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5746\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 57\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5857\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5745\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 58\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5852\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5735\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 59\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5856\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5738\u001b[0m\n",
      "\u001b[34mEpoch 60\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5852\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5749\u001b[0m\n",
      "\u001b[34mEpoch 61\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5852\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5741\u001b[0m\n",
      "\u001b[34mEpoch 62\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5850\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5738\u001b[0m\n",
      "\u001b[34mEpoch 63\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5851\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5739\u001b[0m\n",
      "\u001b[34mEpoch 64\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5846\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5736\u001b[0m\n",
      "\u001b[34mEpoch 65\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5846\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5745\u001b[0m\n",
      "\u001b[34mEpoch 66\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5848\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5735\u001b[0m\n",
      "\u001b[34mEpoch 67\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5845\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5735\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 68\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5846\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5729\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 69\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5844\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5734\u001b[0m\n",
      "\u001b[34mEpoch 70\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5846\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5737\u001b[0m\n",
      "\u001b[34mEpoch 71\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5844\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5728\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 72\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5840\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5737\u001b[0m\n",
      "\u001b[34mEpoch 73\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5844\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5747\u001b[0m\n",
      "\u001b[34mEpoch 74\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5846\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5733\u001b[0m\n",
      "\u001b[34mEpoch 75\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5844\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5733\u001b[0m\n",
      "\u001b[34mEpoch 76\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5837\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5734\u001b[0m\n",
      "\u001b[34mEpoch 77\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5837\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5722\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 78\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5836\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5739\u001b[0m\n",
      "\u001b[34mEpoch 79\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5836\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5723\u001b[0m\n",
      "\u001b[34mEpoch 80\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5835\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5717\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 81\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5836\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5734\u001b[0m\n",
      "\u001b[34mEpoch 82\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5839\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5723\u001b[0m\n",
      "\u001b[34mEpoch 83\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5841\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5727\u001b[0m\n",
      "\u001b[34mEpoch 84\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5840\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5723\u001b[0m\n",
      "\u001b[34mEpoch 85\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5835\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5733\u001b[0m\n",
      "\u001b[34mEpoch 86\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5836\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5728\u001b[0m\n",
      "\u001b[34mEpoch 87\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5832\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5747\u001b[0m\n",
      "\u001b[34mEpoch 88\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5838\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5738\u001b[0m\n",
      "\u001b[34mEpoch 89\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5837\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5734\u001b[0m\n",
      "\u001b[34mEpoch 90\n",
      "  Train: model on cuda\u001b[0m\n",
      "\n",
      "2020-07-21 09:17:57 Uploading - Uploading generated training model\n",
      "2020-07-21 09:17:57 Completed - Training job completed\n",
      "\u001b[34m  Train: Average loss: 0.5836\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5728\n",
      "  Validation loss didn't improve over last 10 epochs. Stopping\u001b[0m\n",
      "\u001b[34mBest training loss: 0.5832 in epoch 87\u001b[0m\n",
      "\u001b[34mBest validation loss: 0.5717 in epoch 80\u001b[0m\n",
      "\u001b[34m[2020-07-21 09:17:46.966 algo-1:43 INFO utils.py:25] The end of training job file will not be written for jobs running under SageMaker.\u001b[0m\n",
      "\u001b[34m2020-07-21 09:17:47,490 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "Training seconds: 2469\n",
      "Billable seconds: 2469\n",
      "CPU times: user 5.6 s, sys: 184 ms, total: 5.78 s\n",
      "Wall time: 44min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Instantiate a pytorch estimator using default hyperparameters.\n",
    "pyt_estimator = PyTorch(entry_point='train.py',\n",
    "                        source_dir='source',\n",
    "                        role=role,\n",
    "                        train_instance_count=1,\n",
    "                        train_instance_type=train_instance_type,\n",
    "                        output_path=pyt_output_path,\n",
    "                        sagemaker_session=sagemaker_session,\n",
    "                        framework_version='1.4',\n",
    "                        enable_sagemaker_metrics=True,\n",
    "                        hyperparameters={\n",
    "                            'input-dim': test_X.shape[1],\n",
    "                            'scalers-config': scalers_config_location,\n",
    "                            'batch-size': 300,\n",
    "                            'hidden-dim': 680,\n",
    "                            'epochs': 100,\n",
    "                            'dropout-rate': 0.3,\n",
    "                            'lr': 0.001,\n",
    "                            'optimizer': 'adam',\n",
    "                            'weigth-init': 'uniform'\n",
    "                        })\n",
    "\n",
    "pyt_estimator.fit({'train': s3_input_train, 'validation': s3_input_val})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model\n",
    "\n",
    "Before training part of the data set was put aside for final evaluation of the model using a batch transform job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to evaluate the endpoint on test data\n",
    "# returns a variety of model metrics\n",
    "def evaluate(predictor, test_features, test_labels, verbose=True):\n",
    "    \"\"\"\n",
    "    Evaluate a model on a test set given the prediction endpoint.  \n",
    "    Return binary classification metrics.\n",
    "    :param predictor: A prediction endpoint\n",
    "    :param test_features: Test features\n",
    "    :param test_labels: Class labels for test data\n",
    "    :param verbose: If True, prints a table of all performance metrics\n",
    "    :return: A dictionary of performance metrics.\n",
    "    \"\"\"\n",
    "    \n",
    "    # running predictions. Testing showed that max payload for predictor is \n",
    "    # 25000 items. Test set will be split and results joined back.\n",
    "    test_preds = []\n",
    "    nr_subarrays = test_features.shape[0] / 20_000\n",
    "    test_features_split = np.array_split(test_features, nr_subarrays)\n",
    "    for subarray in test_features_split:\n",
    "        print(\"prediction...\")\n",
    "        test_preds.append(np.squeeze(np.round(predictor.predict(subarray))))\n",
    "        \n",
    "    test_preds = np.concatenate(test_preds)\n",
    "    \n",
    "    # calculate true positives, false positives, true negatives, false negatives\n",
    "    tp = np.logical_and(test_labels, test_preds).sum()\n",
    "    fp = np.logical_and(1-test_labels, test_preds).sum()\n",
    "    tn = np.logical_and(1-test_labels, 1-test_preds).sum()\n",
    "    fn = np.logical_and(test_labels, 1-test_preds).sum()\n",
    "    \n",
    "    # calculate binary classification metrics\n",
    "    recall = tp / (tp + fn)\n",
    "    precision = tp / (tp + fp)\n",
    "    accuracy = (tp + tn) / (tp + fp + tn + fn)\n",
    "    auc = roc_auc_score(test_labels, test_preds)\n",
    "    \n",
    "    # print metrics\n",
    "    if verbose:\n",
    "        print(pd.crosstab(test_labels, test_preds, rownames=['actuals'], colnames=['predictions']))\n",
    "        print(\"\\n{:<11} {:.4f}\".format('Recall:', recall))\n",
    "        print(\"{:<11} {:.4f}\".format('Precision:', precision))\n",
    "        print(\"{:<11} {:.4f}\".format('Accuracy:', accuracy))\n",
    "        print(\"{:<11} {:.4f}\".format('AUC:', auc))\n",
    "        print()\n",
    "                \n",
    "    return {'TP': tp, 'FP': fp, 'FN': fn, 'TN': tn, \n",
    "            'Precision': precision, 'Recall': recall, 'Accuracy': accuracy, 'AUC': auc}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n",
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!CPU times: user 868 ms, sys: 41.8 ms, total: 910 ms\n",
      "Wall time: 7min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "\n",
    "# Create a model from the trained estimator data and point to the prediction script\n",
    "model = PyTorchModel(model_data=pyt_estimator.model_data,\n",
    "                     entry_point='predict.py',\n",
    "                     source_dir='source',\n",
    "                     role=role,\n",
    "                     framework_version='1.4')\n",
    "\n",
    "predictor = model.deploy(initial_instance_count=1, instance_type=deploy_instance_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction...\n",
      "prediction...\n",
      "prediction...\n",
      "prediction...\n",
      "prediction...\n",
      "prediction...\n",
      "prediction...\n",
      "predictions    0.0    1.0\n",
      "actuals                  \n",
      "0            45691  24559\n",
      "1            20140  59610\n",
      "\n",
      "Recall:     0.7475\n",
      "Precision:  0.7082\n",
      "Accuracy:   0.7020\n",
      "AUC:        0.6989\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metrics = evaluate(predictor, test_X, test_y, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model achieves an AUC score of **0.6989**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploying the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()\n",
    "model.delete_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rest API deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch-inference-2020-07-21-09-32-04-914\n"
     ]
    }
   ],
   "source": [
    "# Code for using predictor from Lambda\n",
    "print(predictor.endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below shows the code of the Lambda Function, which should be created with following properties\n",
    "\n",
    "* Runtime: python 3.6\n",
    "* Memory Size: 1024MB\n",
    "* Timeout: 29sec\n",
    "* Environment variable: SM_ENDPOINT set to the output of the above cell (`predictor.endpoint`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mboto3\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mlambda_handler\u001b[39;49;00m(event, context):\n",
      "    \n",
      "    data = event[\u001b[33m'\u001b[39;49;00m\u001b[33mbody\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "\n",
      "    \u001b[37m# The SageMaker runtime is what allows us to invoke the endpoint that we've created.\u001b[39;49;00m\n",
      "    runtime = boto3.Session().client(\u001b[33m'\u001b[39;49;00m\u001b[33msagemaker-runtime\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# Now we use the SageMaker runtime to invoke our endpoint, sending the review we were given\u001b[39;49;00m\n",
      "    response = runtime.invoke_endpoint(EndpointName=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_ENDPOINT\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m],\n",
      "                                       ContentType=\u001b[33m'\u001b[39;49;00m\u001b[33mtext/csv\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                                       Accept=\u001b[33m'\u001b[39;49;00m\u001b[33mtext/csv\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                                       Body=data)\n",
      "\n",
      "    \u001b[37m# The response is an HTTP response whose body contains the result of our inference\u001b[39;49;00m\n",
      "    result = response[\u001b[33m'\u001b[39;49;00m\u001b[33mBody\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].read().decode(\u001b[33m'\u001b[39;49;00m\u001b[33mutf-8\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).split()\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m {\n",
      "        \u001b[33m'\u001b[39;49;00m\u001b[33mstatusCode\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m : \u001b[34m200\u001b[39;49;00m,\n",
      "        \u001b[33m'\u001b[39;49;00m\u001b[33mheaders\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m : { \u001b[33m'\u001b[39;49;00m\u001b[33mContent-Type\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m : \u001b[33m'\u001b[39;49;00m\u001b[33mtext/csv\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m },\n",
      "        \u001b[33m'\u001b[39;49;00m\u001b[33mbody\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m : \u001b[36mstr\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m,\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.join(result))\n",
      "    }\n"
     ]
    }
   ],
   "source": [
    "!pygmentize lambda/lambda.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the Lambda Function is created, A Rest API should be created that uses this Lambda function with proxy integration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
