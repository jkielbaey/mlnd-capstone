{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Higgs Boson - Collision event classification\n",
    "\n",
    "## Deep Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload data to S3\n",
    "\n",
    "The data files have already been prepared locally on the storage of the Sagemaker Notebook instance. Next we'll upload them onto the default S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import accuracy_score, recall_score, roc_auc_score\n",
    "\n",
    "import os\n",
    "import time\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in remote mode.\n"
     ]
    }
   ],
   "source": [
    "# SageMaker session and role\n",
    "LOCAL_MODE = False\n",
    "if LOCAL_MODE:\n",
    "    # During model development it's more efficient to use SageMaker local model.\n",
    "    # https://aws.amazon.com/blogs/machine-learning/use-the-amazon-sagemaker-local-mode-to-train-on-your-notebook-instance/\n",
    "    sagemaker_session = sagemaker.LocalSession()\n",
    "\n",
    "    # If notebook instance does not have GPUs, change to 'local'.\n",
    "    train_instance_type = 'local'\n",
    "    deploy_instance_type = 'local'\n",
    "    print('Running in local mode.')\n",
    "else:\n",
    "    sagemaker_session = sagemaker.Session()\n",
    "    train_instance_type = 'ml.p2.xlarge'\n",
    "    deploy_instance_type = 'ml.m5.large'\n",
    "    print('Running in remote mode.')\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# Use the default Sagemaker an S3 bucket\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "# Prefix for objects in S3 bucket\n",
    "prefix = 'capstone/nn_250000'\n",
    "\n",
    "# Location of data files on notebook storage.\n",
    "data_dir = '../data/250000'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 locations: train s3://sagemaker-eu-west-1-019518462631/capstone/nn_250000/train.csv / validation s3://sagemaker-eu-west-1-019518462631/capstone/nn_250000/val.csv\n"
     ]
    }
   ],
   "source": [
    "# Actual upload of the data files.\n",
    "train_path = sagemaker_session.upload_data(data_dir+'/train.csv', bucket=bucket, key_prefix=prefix)\n",
    "val_path = sagemaker_session.upload_data(data_dir+'/val.csv', bucket=bucket, key_prefix=prefix)\n",
    "\n",
    "s3_input_train = sagemaker.s3_input(s3_data=train_path, content_type='csv')\n",
    "s3_input_val = sagemaker.s3_input(s3_data=val_path, content_type='csv')\n",
    "\n",
    "print(\"S3 locations: train {} / validation {}\".format(train_path, val_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 locations: test s3://sagemaker-eu-west-1-019518462631/capstone/nn_250000/test-2.csv\n"
     ]
    }
   ],
   "source": [
    "# Read the test data.\n",
    "test_df = pd.read_csv(os.path.join(data_dir, 'test.csv'), header=None)\n",
    "\n",
    "# Seperate labels and features\n",
    "test_y = test_df.iloc[:,0]\n",
    "test_X = test_df.iloc[:, 1:]\n",
    "\n",
    "# Dump the test features and upload to S3.\n",
    "test_file = 'test-2.csv'\n",
    "pd.DataFrame(test_X).to_csv(os.path.join(data_dir, test_file), header=False, index=False)\n",
    "test_path = sagemaker_session.upload_data(os.path.join(data_dir, test_file), bucket=bucket, key_prefix=prefix)\n",
    "\n",
    "print(\"S3 locations: test {}\".format(test_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ../data/250000/scalers.pkl to s3://sagemaker-eu-west-1-019518462631/capstone/nn_250000/scalers.pkl\n",
      "S3 locations: scalers s3://sagemaker-eu-west-1-019518462631/capstone/nn_250000/scalers.pkl\n"
     ]
    }
   ],
   "source": [
    "# Upload sklearn scalers model.\n",
    "scalers_path = \"s3://{}/{}/scalers.pkl\".format(bucket, prefix)\n",
    "!aws s3 cp $data_dir/scalers.pkl $scalers_path\n",
    "\n",
    "print(\"S3 locations: scalers {}\".format(scalers_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "capstone/nn_250000/test-2.csv\n",
      "capstone/nn_250000/test-2.csv.out\n",
      "capstone/nn_250000/train.csv\n",
      "capstone/nn_250000/val.csv\n"
     ]
    }
   ],
   "source": [
    "# Check if files have been uploaded\n",
    "for obj in boto3.resource('s3').Bucket(bucket).objects.all():\n",
    "    if prefix in obj.key and '.csv' in obj.key:\n",
    "        print(obj.key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep learning model\n",
    "\n",
    "Next I wanted to evaluate the performance of a deep neural network.\n",
    "\n",
    "After several try-outs I'm using following model.\n",
    "\n",
    "* Input layer\n",
    "    * Fully connected layer\n",
    "    * Tanh activation function\n",
    "    * Batch normalization\n",
    "    * Drop out\n",
    "\n",
    "* 3 hidden layers\n",
    "    * Fully connected layer\n",
    "    * Tanh activation function\n",
    "    * Batch normalization\n",
    "    * Drop out\n",
    "\n",
    "* Output layer\n",
    "    * Fully connected layer (with 1 output node)\n",
    "    * Sigmoid activation function\n",
    "    \n",
    "The cells below show the code of the model and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnn\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mfunctional\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mF\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mNet\u001b[39;49;00m(nn.Module):\n",
      "    \n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, input_dim, hidden_dim, init_weights=\u001b[33m'\u001b[39;49;00m\u001b[33mnone\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, dropout_rate=\u001b[34m0.5\u001b[39;49;00m):\n",
      "        \u001b[33m'''Defines layers of a neural network.\u001b[39;49;00m\n",
      "\u001b[33m           :param input_dim: Number of input features\u001b[39;49;00m\n",
      "\u001b[33m           :param hidden\u001b[39;49;00m\n",
      "\u001b[33m           :param hidden_dim: Size of hidden layer(s)\u001b[39;49;00m\n",
      "\u001b[33m           :param output_dim: Number of outputs\u001b[39;49;00m\n",
      "\u001b[33m         '''\u001b[39;49;00m\n",
      "        \u001b[36msuper\u001b[39;49;00m(Net, \u001b[36mself\u001b[39;49;00m).\u001b[32m__init__\u001b[39;49;00m()\n",
      "        \n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mMaking Net with input \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m, hidden \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(input_dim, hidden_dim))\n",
      "\n",
      "        \u001b[36mself\u001b[39;49;00m.model = nn.Sequential(\n",
      "            \n",
      "            \u001b[37m# Input layer\u001b[39;49;00m\n",
      "            nn.Linear(input_dim, hidden_dim),\n",
      "            nn.Tanh(),\n",
      "            nn.BatchNorm1d(hidden_dim),\n",
      "            nn.Dropout(dropout_rate),\n",
      "\n",
      "            \u001b[37m# Hidden layers\u001b[39;49;00m\n",
      "            nn.Linear(hidden_dim, hidden_dim),\n",
      "            nn.Tanh(),\n",
      "            nn.BatchNorm1d(hidden_dim),\n",
      "            nn.Dropout(dropout_rate),\n",
      "\n",
      "            nn.Linear(hidden_dim, hidden_dim),\n",
      "            nn.Tanh(),\n",
      "            nn.BatchNorm1d(hidden_dim),\n",
      "            nn.Dropout(dropout_rate),\n",
      "\n",
      "            nn.Linear(hidden_dim, hidden_dim),\n",
      "            nn.Tanh(),\n",
      "            nn.BatchNorm1d(hidden_dim),\n",
      "            nn.Dropout(dropout_rate),\n",
      "                       \n",
      "            \u001b[37m# Output layer\u001b[39;49;00m\n",
      "            nn.Linear(hidden_dim, \u001b[34m1\u001b[39;49;00m),\n",
      "        )\n",
      "        \u001b[36mself\u001b[39;49;00m.sig = nn.Sigmoid()\n",
      "\n",
      "        \u001b[34mif\u001b[39;49;00m init_weights == \u001b[33m'\u001b[39;49;00m\u001b[33mxavier_uniform\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
      "            \u001b[36mself\u001b[39;49;00m.model.apply(Net.init_weights_xavier_uniform)\n",
      "        \u001b[34melif\u001b[39;49;00m init_weights == \u001b[33m'\u001b[39;49;00m\u001b[33mxavier_normal\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
      "            \u001b[36mself\u001b[39;49;00m.model.apply(Net.init_weights_xavier_normal)\n",
      "        \u001b[34melif\u001b[39;49;00m init_weights == \u001b[33m'\u001b[39;49;00m\u001b[33muniform\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
      "            \u001b[36mself\u001b[39;49;00m.model.apply(Net.init_weights_uniform)\n",
      "\n",
      "    \u001b[90m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32minit_weights_xavier_uniform\u001b[39;49;00m(model):\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[36mtype\u001b[39;49;00m(model) == nn.Linear:\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mSetting initial weights on \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(\u001b[36mstr\u001b[39;49;00m(model)))\n",
      "            torch.nn.init.xavier_uniform_(model.weight)\n",
      "            \u001b[34mif\u001b[39;49;00m model.bias \u001b[35mis\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m:\n",
      "                model.bias.data.fill_(\u001b[34m0\u001b[39;49;00m)    \n",
      "\n",
      "    \u001b[90m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32minit_weights_xavier_normal\u001b[39;49;00m(model):\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[36mtype\u001b[39;49;00m(model) == nn.Linear:\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mSetting initial weights on \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(\u001b[36mstr\u001b[39;49;00m(model)))\n",
      "            torch.nn.init.xavier_normal_(model.weight)\n",
      "            \u001b[34mif\u001b[39;49;00m model.bias \u001b[35mis\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m:\n",
      "                model.bias.data.fill_(\u001b[34m0\u001b[39;49;00m)    \n",
      "\n",
      "    \u001b[90m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32minit_weights_uniform\u001b[39;49;00m(model):\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[36mtype\u001b[39;49;00m(model) == nn.Linear:\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mSetting initial weights on \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(\u001b[36mstr\u001b[39;49;00m(model)))\n",
      "            n = model.in_features\n",
      "            y = \u001b[34m1.0\u001b[39;49;00m/np.sqrt(n)\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mY=\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(y))\n",
      "            model.weight.data.uniform_(-y, y)\n",
      "            \u001b[34mif\u001b[39;49;00m model.bias \u001b[35mis\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m:\n",
      "                model.bias.data.fill_(\u001b[34m0\u001b[39;49;00m)    \n",
      "\n",
      "\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mforward\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, x):\n",
      "        \u001b[33m'''Feedforward behavior of the net.\u001b[39;49;00m\n",
      "\u001b[33m           :param x: A batch of input features\u001b[39;49;00m\n",
      "\u001b[33m           :return: A single, sigmoid activated value\u001b[39;49;00m\n",
      "\u001b[33m         '''\u001b[39;49;00m\n",
      "\n",
      "        x = \u001b[36mself\u001b[39;49;00m.model(x)\n",
      "        \u001b[34mreturn\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.sig(x)\n"
     ]
    }
   ],
   "source": [
    "!pygmentize source/model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36m__future__\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m print_function \u001b[37m# future proof\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mboto3\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mre\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjoblib\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# pytorch\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnn\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36moptim\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36moptim\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdata\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# import model\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mmodel\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Net\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_fn\u001b[39;49;00m(model_dir):\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mLoading model.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[34mglobal\u001b[39;49;00m scalers\n",
      "\n",
      "    \u001b[37m# First, load the parameters used to create the model.\u001b[39;49;00m\n",
      "    model_info = {}\n",
      "    model_info_path = os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel_info.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(model_info_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
      "        model_info = torch.load(f)\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mmodel_info: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(model_info))\n",
      "\n",
      "    \u001b[37m# Determine the device and construct the model.\u001b[39;49;00m\n",
      "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    model = Net(model_info[\u001b[33m'\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], \n",
      "                model_info[\u001b[33m'\u001b[39;49;00m\u001b[33mhidden_dim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    \n",
      "    scalers = model_info[\u001b[33m'\u001b[39;49;00m\u001b[33mscalers\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "\n",
      "    \u001b[37m# Load the stored model parameters.\u001b[39;49;00m\n",
      "    model_path = os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(model_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
      "        model.load_state_dict(torch.load(f))\n",
      "    \n",
      "    \u001b[34mreturn\u001b[39;49;00m model.to(device)\n",
      "\n",
      "\n",
      "\u001b[37m# Load the data from a csv file\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_get_data_loader\u001b[39;49;00m(batch_size, data_dir, data_file):\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mGet data loader for file \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(data_file))\n",
      "\n",
      "    data = pd.read_csv(os.path.join(data_dir, data_file), header=\u001b[34mNone\u001b[39;49;00m, names=\u001b[34mNone\u001b[39;49;00m)\n",
      "    y = torch.from_numpy(data[[\u001b[34m0\u001b[39;49;00m]].values).float().squeeze()\n",
      "    x = torch.from_numpy(data.drop([\u001b[34m0\u001b[39;49;00m], axis=\u001b[34m1\u001b[39;49;00m).values).float()\n",
      "    ds = torch.utils.data.TensorDataset(x, y)\n",
      "    \u001b[34mreturn\u001b[39;49;00m torch.utils.data.DataLoader(ds, batch_size=batch_size, shuffle=\u001b[34mTrue\u001b[39;49;00m)\n",
      "\n",
      "    \n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtrain_epoch\u001b[39;49;00m(model, device, train_loader, optimizer, criterion, epoch):\n",
      "    \u001b[33m\"\"\" Perform training for a single epoch.\u001b[39;49;00m\n",
      "\u001b[33m    \u001b[39;49;00m\n",
      "\u001b[33m    :param model: PyTorch model to train\u001b[39;49;00m\n",
      "\u001b[33m    :param device: Where the model and data should be loaded (cpu or gpu)\u001b[39;49;00m\n",
      "\u001b[33m    :param train_loader: PyTorch DataLoader used during training\u001b[39;49;00m\n",
      "\u001b[33m    :param optimizer: The optimizer used during training\u001b[39;49;00m\n",
      "\u001b[33m    :param criterion: The loss function used for training\u001b[39;49;00m\n",
      "\u001b[33m    :param epoch: Epoch that the model is being trained for\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "    model.train()\n",
      "    model.to(device)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m  Train: model on \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(device))\n",
      "    \n",
      "    \n",
      "    total_loss = \u001b[34m0\u001b[39;49;00m\n",
      "\n",
      "    \u001b[34mfor\u001b[39;49;00m batch_idx, (batch_X, batch_y) \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(train_loader, \u001b[34m1\u001b[39;49;00m):\n",
      "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
      "\n",
      "        optimizer.zero_grad()\n",
      "        output = model(batch_X).view(-\u001b[34m1\u001b[39;49;00m)\n",
      "        loss = criterion(output, batch_y)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        total_loss += loss.data.item()\n",
      "        \n",
      "    avg_loss = total_loss/\u001b[36mlen\u001b[39;49;00m(train_loader)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m  Train: Average loss: \u001b[39;49;00m\u001b[33m{:.4f}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(avg_loss))\n",
      "    \n",
      "    \u001b[34mreturn\u001b[39;49;00m avg_loss\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtest_model\u001b[39;49;00m(model, device, test_loader, criterion):\n",
      "    model.eval()\n",
      "    model.to(device)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m  Validation: model on \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(device))\n",
      "\n",
      "    test_losses = []\n",
      "    total_loss = \u001b[34m0\u001b[39;49;00m\n",
      "\n",
      "    \u001b[34mwith\u001b[39;49;00m torch.no_grad():\n",
      "        \u001b[34mfor\u001b[39;49;00m batch_idx, (batch_X, batch_y) \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(test_loader, \u001b[34m1\u001b[39;49;00m):\n",
      "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
      "            output = model(batch_X).view(-\u001b[34m1\u001b[39;49;00m)\n",
      "\n",
      "            test_loss = criterion(output, batch_y)\n",
      "            total_loss += test_loss.data.item()\n",
      "\n",
      "    avg_test_loss = total_loss / \u001b[36mlen\u001b[39;49;00m(test_loader)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m  Validation: Average loss: \u001b[39;49;00m\u001b[33m{:.4f}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(avg_test_loss))\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m avg_test_loss\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtrain_model\u001b[39;49;00m(model, device, train_loader, test_loader, optimizer, criterion, epochs, model_dir):\n",
      "    best_val_loss = \u001b[34mNone\u001b[39;49;00m\n",
      "    best_val_loss_epoch = -\u001b[34m1\u001b[39;49;00m\n",
      "    best_train_loss = \u001b[34mNone\u001b[39;49;00m\n",
      "    best_train_loss_epoch = -\u001b[34m1\u001b[39;49;00m\n",
      "    \u001b[34mfor\u001b[39;49;00m epoch \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(\u001b[34m1\u001b[39;49;00m, epochs + \u001b[34m1\u001b[39;49;00m):\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mEpoch \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(epoch))\n",
      "        train_loss = train_epoch(model, device, train_loader, optimizer, criterion, epoch)\n",
      "        val_loss = test_model(model, device, test_loader, criterion)\n",
      "\n",
      "        \u001b[34mif\u001b[39;49;00m best_train_loss \u001b[35mis\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m \u001b[35mor\u001b[39;49;00m train_loss < best_train_loss:\n",
      "            best_train_loss = train_loss\n",
      "            best_train_loss_epoch = epoch\n",
      "        \u001b[34mif\u001b[39;49;00m best_val_loss \u001b[35mis\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m \u001b[35mor\u001b[39;49;00m val_loss < best_val_loss:\n",
      "            best_val_loss = val_loss\n",
      "            best_val_loss_epoch = epoch\n",
      "            save_model(model, model_dir)\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m  Model performed better on validation...Saved.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "            \n",
      "        \u001b[34mif\u001b[39;49;00m val_loss > best_val_loss \u001b[35mand\u001b[39;49;00m epoch - best_val_loss_epoch >= \u001b[34m10\u001b[39;49;00m:\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m  Validation loss didn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mt improve over last 10 epochs. Stopping\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "            \u001b[34mbreak\u001b[39;49;00m\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mBest training loss: \u001b[39;49;00m\u001b[33m{:.4f}\u001b[39;49;00m\u001b[33m in epoch \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(best_train_loss, best_train_loss_epoch))\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mBest validation loss: \u001b[39;49;00m\u001b[33m{:.4f}\u001b[39;49;00m\u001b[33m in epoch \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(best_val_loss, best_val_loss_epoch))\n",
      "\n",
      "\n",
      "\u001b[37m# Provided model saving functions\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32msave_model\u001b[39;49;00m(model, model_dir):\n",
      "    path = os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# save state dictionary\u001b[39;49;00m\n",
      "    torch.save(model.cpu().state_dict(), path)\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32msave_model_params\u001b[39;49;00m(model_info, model_dir):\n",
      "    model_info_path = os.path.join(args.model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel_info.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mSaving model params in \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(model_info_path))\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(model_info_path, \u001b[33m'\u001b[39;49;00m\u001b[33mwb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
      "        torch.save(model_info, f)\n",
      "\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m'\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
      "    \u001b[37m# All of the model parameters and training parameters are sent as arguments\u001b[39;49;00m\n",
      "    \u001b[37m# when this script is executed, during a training job\u001b[39;49;00m\n",
      "    \n",
      "    \u001b[37m# Here we set up an argument parser to easily access the parameters\u001b[39;49;00m\n",
      "    parser = argparse.ArgumentParser()\n",
      "\n",
      "    \u001b[37m# SageMaker parameters, like the directories for training data and saving models; set automatically\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--hosts\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mlist\u001b[39;49;00m, default=json.loads(os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_HOSTS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]))\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--current-host\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CURRENT_HOST\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--model-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--train-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAIN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--validation-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_VALIDATION\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--scalers-config\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[37m# Training Parameters\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--batch-size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m64\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33minput batch size for training (default: 64)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m50\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mnumber of epochs to train (default: 10)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--optimizer\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m'\u001b[39;49;00m\u001b[33madam\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mOPTIM\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33moptimizer (default: adam)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--lr\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.001\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mLR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mlearning rate (default: 0.001)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--momentum\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mM\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mlearning rate (default: 0.001)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--seed\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m42\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mrandom seed (default: 42)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--weigth-init\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m'\u001b[39;49;00m\u001b[33mnone\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mweight init method (default: pytorch default)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "  \n",
      "    \u001b[37m# Model parameters\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--input-dim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m2\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mIN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33minput batch size for training (default: 2)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--hidden-dim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m50\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mH\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33minput batch size for training (default: 50)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--dropout-rate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.2\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mDR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mdropout rate (default: 0.2)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \n",
      "    args = parser.parse_args()\n",
      "\n",
      "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mModel will be trained on \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(device))\n",
      "    \n",
      "    \u001b[37m# set the seed for generating random numbers\u001b[39;49;00m\n",
      "    torch.manual_seed(args.seed)\n",
      "    \u001b[34mif\u001b[39;49;00m torch.cuda.is_available():\n",
      "        torch.cuda.manual_seed(args.seed)\n",
      "        \n",
      "    m = re.match(\u001b[33m'\u001b[39;49;00m\u001b[33ms3://([^/]+)/(.+)$\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, args.scalers_config)\n",
      "    \u001b[34mif\u001b[39;49;00m m \u001b[35mis\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m:\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mScaler config param invalid (\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m).\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(args.scalers_config))\n",
      "        sys.exit(\u001b[34m1\u001b[39;49;00m)\n",
      "\n",
      "    bucket = m.group(\u001b[34m1\u001b[39;49;00m)\n",
      "    key = m.group(\u001b[34m2\u001b[39;49;00m)\n",
      "\n",
      "    boto3.resource(\u001b[33m'\u001b[39;49;00m\u001b[33ms3\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).Bucket(bucket).download_file(key, \u001b[33m'\u001b[39;49;00m\u001b[33mscaler.pkl\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mscaler.pkl\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
      "        scalers = joblib.load(f)\n",
      "        \n",
      "    \u001b[37m# get train loader\u001b[39;49;00m\n",
      "    train_loader = _get_data_loader(args.batch_size, args.train_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mtrain.csv\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    test_loader = _get_data_loader(args.batch_size, args.validation_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mval.csv\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \n",
      "    model = Net(args.input_dim, args.hidden_dim, init_weights=args.weigth_init, dropout_rate=args.dropout_rate).to(device)\n",
      "    \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mShow model:\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[36mprint\u001b[39;49;00m(model)\n",
      "\n",
      "    model_info = {\n",
      "        \u001b[33m'\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: args.input_dim,\n",
      "        \u001b[33m'\u001b[39;49;00m\u001b[33mhidden_dim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: args.hidden_dim,\n",
      "        \u001b[33m'\u001b[39;49;00m\u001b[33mdropout_rate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: args.dropout_rate,\n",
      "        \u001b[33m'\u001b[39;49;00m\u001b[33mscalers\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: scalers\n",
      "    }\n",
      "    save_model_params(model_info, args.model_dir)\n",
      "\n",
      "    optimzers = {\n",
      "        \u001b[33m'\u001b[39;49;00m\u001b[33madam\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: optim.Adam(model.parameters(), lr=args.lr),\n",
      "        \u001b[33m'\u001b[39;49;00m\u001b[33msgd\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n",
      "    }\n",
      "    optimizer = optimzers[args.optimizer]\n",
      "    criterion = nn.BCELoss()\n",
      "        \n",
      "    train_model(model, device, train_loader, test_loader, optimizer, criterion, args.epochs, args.model_dir)\n"
     ]
    }
   ],
   "source": [
    "!pygmentize source/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial PyTorch estimator\n",
    "\n",
    "Before diving into Hyperparameter tuning, let's start by training a NN model using default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 locations: output path s3://sagemaker-eu-west-1-019518462631/capstone/nn_250000\n"
     ]
    }
   ],
   "source": [
    "# import a PyTorch wrapper\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "scalers_config_location = '{}/scalers.pkl'.format(pyt_output_path)\n",
    "\n",
    "# specify an output path\n",
    "pyt_output_path = \"s3://{}/{}\".format(bucket, prefix)\n",
    "print(\"S3 locations: output path {}\".format(pyt_output_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n",
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-19 13:13:39 Starting - Starting the training job...\n",
      "2020-07-19 13:13:41 Starting - Launching requested ML instances.........\n",
      "2020-07-19 13:15:13 Starting - Preparing the instances for training.........\n",
      "2020-07-19 13:16:47 Downloading - Downloading input data...\n",
      "2020-07-19 13:17:30 Training - Downloading the training image...........\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2020-07-19 13:19:21,137 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2020-07-19 13:19:21,161 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2020-07-19 13:19:21,802 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-07-19 13:19:22,093 sagemaker-containers INFO     Module default_user_module_name does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2020-07-19 13:19:22,093 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2020-07-19 13:19:22,093 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2020-07-19 13:19:22,094 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python -m pip install . -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing /tmp/tmpc20lhhpw/module_dir\u001b[0m\n",
      "\u001b[34mCollecting scikit-learn==0.22.1\n",
      "  Downloading scikit_learn-0.22.1-cp36-cp36m-manylinux1_x86_64.whl (7.0 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.11.0 in /opt/conda/lib/python3.6/site-packages (from scikit-learn==0.22.1->-r requirements.txt (line 1)) (1.16.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.6/site-packages (from scikit-learn==0.22.1->-r requirements.txt (line 1)) (0.15.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy>=0.17.0 in /opt/conda/lib/python3.6/site-packages (from scikit-learn==0.22.1->-r requirements.txt (line 1)) (1.2.2)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: default-user-module-name\n",
      "  Building wheel for default-user-module-name (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for default-user-module-name (setup.py): finished with status 'done'\n",
      "  Created wheel for default-user-module-name: filename=default_user_module_name-1.0.0-py2.py3-none-any.whl size=22628 sha256=3fb044f3bed10d46c0efc1d3aa9b8492c0c8c806c75bc9f66624457baf8e8cc5\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-wjwqzj0b/wheels/c0/71/31/d1a6ddae7b79309b80fcfd7c8a92f748da93ac8c64ee3d41f3\u001b[0m\n",
      "\u001b[34mSuccessfully built default-user-module-name\u001b[0m\n",
      "\u001b[34mInstalling collected packages: scikit-learn, default-user-module-name\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.21.2\u001b[0m\n",
      "\u001b[34m    Uninstalling scikit-learn-0.21.2:\n",
      "      Successfully uninstalled scikit-learn-0.21.2\u001b[0m\n",
      "\n",
      "2020-07-19 13:19:19 Training - Training image download completed. Training in progress.\u001b[34mSuccessfully installed default-user-module-name-1.0.0 scikit-learn-0.22.1\u001b[0m\n",
      "\u001b[34m2020-07-19 13:19:27,482 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"validation\": \"/opt/ml/input/data/validation\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"input-dim\": 17,\n",
      "        \"scalers-config\": \"s3://sagemaker-eu-west-1-019518462631/capstone/nn_250000/scalers.pkl\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"validation\": {\n",
      "            \"ContentType\": \"csv\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"ContentType\": \"csv\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2020-07-19-13-13-39-243\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-eu-west-1-019518462631/pytorch-training-2020-07-19-13-13-39-243/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"input-dim\":17,\"scalers-config\":\"s3://sagemaker-eu-west-1-019518462631/capstone/nn_250000/scalers.pkl\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"ContentType\":\"csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"ContentType\":\"csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\",\"validation\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-eu-west-1-019518462631/pytorch-training-2020-07-19-13-13-39-243/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\",\"validation\":\"/opt/ml/input/data/validation\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"input-dim\":17,\"scalers-config\":\"s3://sagemaker-eu-west-1-019518462631/capstone/nn_250000/scalers.pkl\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"ContentType\":\"csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"ContentType\":\"csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2020-07-19-13-13-39-243\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-eu-west-1-019518462631/pytorch-training-2020-07-19-13-13-39-243/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--input-dim\",\"17\",\"--scalers-config\",\"s3://sagemaker-eu-west-1-019518462631/capstone/nn_250000/scalers.pkl\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VALIDATION=/opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_INPUT-DIM=17\u001b[0m\n",
      "\u001b[34mSM_HP_SCALERS-CONFIG=s3://sagemaker-eu-west-1-019518462631/capstone/nn_250000/scalers.pkl\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python train.py --input-dim 17 --scalers-config s3://sagemaker-eu-west-1-019518462631/capstone/nn_250000/scalers.pkl\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mModel will be trained on cuda\u001b[0m\n",
      "\u001b[34mGet data loader for file train.csv.\u001b[0m\n",
      "\u001b[34mGet data loader for file val.csv.\u001b[0m\n",
      "\u001b[34mMaking Net with input 17, hidden 50\u001b[0m\n",
      "\u001b[34mShow model:\u001b[0m\n",
      "\u001b[34mNet(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=17, out_features=50, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Dropout(p=0.2, inplace=False)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Tanh()\n",
      "    (6): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): Dropout(p=0.2, inplace=False)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Tanh()\n",
      "    (10): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): Dropout(p=0.2, inplace=False)\n",
      "    (12): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (13): Tanh()\n",
      "    (14): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (15): Dropout(p=0.2, inplace=False)\n",
      "    (16): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      "  (sig): Sigmoid()\u001b[0m\n",
      "\u001b[34m)\u001b[0m\n",
      "\u001b[34mSaving model params in /opt/ml/model/model_info.pth\u001b[0m\n",
      "\u001b[34mEpoch 1\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m[2020-07-19 13:19:34.745 algo-1:42 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2020-07-19 13:19:34.746 algo-1:42 INFO hook.py:191] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2020-07-19 13:19:34.746 algo-1:42 INFO hook.py:236] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2020-07-19 13:19:34.746 algo-1:42 INFO state_store.py:67] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2020-07-19 13:19:34.759 algo-1:42 INFO hook.py:376] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2020-07-19 13:19:34.759 algo-1:42 INFO hook.py:437] Hook is writing from the hook with pid: 42\n",
      "\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6814\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6628\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 2\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6622\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6453\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 3\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6534\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6387\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 4\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6492\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6346\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 5\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6454\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6325\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 6\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6429\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6294\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 7\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6414\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6249\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 8\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6395\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6231\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 9\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6392\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6233\u001b[0m\n",
      "\u001b[34mEpoch 10\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6382\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6209\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 11\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6365\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6191\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 12\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6349\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6189\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 13\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6343\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6172\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 14\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6339\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6187\u001b[0m\n",
      "\u001b[34mEpoch 15\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6337\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6166\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 16\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6323\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6172\u001b[0m\n",
      "\u001b[34mEpoch 17\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6315\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6136\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 18\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6319\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6134\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 19\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6309\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6140\u001b[0m\n",
      "\u001b[34mEpoch 20\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6303\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6123\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 21\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6304\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6130\u001b[0m\n",
      "\u001b[34mEpoch 22\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6285\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6113\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 23\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6286\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6136\u001b[0m\n",
      "\u001b[34mEpoch 24\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6285\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6118\u001b[0m\n",
      "\u001b[34mEpoch 25\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6290\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6112\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 26\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6278\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6109\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 27\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6285\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6108\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 28\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6277\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6096\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 29\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6271\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6106\u001b[0m\n",
      "\u001b[34mEpoch 30\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6267\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6099\u001b[0m\n",
      "\u001b[34mEpoch 31\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6270\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6112\u001b[0m\n",
      "\u001b[34mEpoch 32\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6260\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6109\u001b[0m\n",
      "\u001b[34mEpoch 33\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6259\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6093\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 34\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6260\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6092\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 35\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6262\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6097\u001b[0m\n",
      "\u001b[34mEpoch 36\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6250\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6091\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 37\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6252\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6085\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 38\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6249\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6098\u001b[0m\n",
      "\u001b[34mEpoch 39\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6256\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6076\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 40\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6246\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6092\u001b[0m\n",
      "\u001b[34mEpoch 41\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6252\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6101\u001b[0m\n",
      "\u001b[34mEpoch 42\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6243\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6076\u001b[0m\n",
      "\u001b[34mEpoch 43\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6239\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6088\u001b[0m\n",
      "\u001b[34mEpoch 44\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6242\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6085\u001b[0m\n",
      "\u001b[34mEpoch 45\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6243\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6088\u001b[0m\n",
      "\u001b[34mEpoch 46\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6238\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6091\u001b[0m\n",
      "\u001b[34mEpoch 47\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6234\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6083\u001b[0m\n",
      "\u001b[34mEpoch 48\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6240\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6081\u001b[0m\n",
      "\u001b[34mEpoch 49\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6230\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6072\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 50\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6237\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6070\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mBest training loss: 0.6230 in epoch 49\u001b[0m\n",
      "\u001b[34mBest validation loss: 0.6070 in epoch 50\u001b[0m\n",
      "\u001b[34m[2020-07-19 13:36:56.967 algo-1:42 INFO utils.py:25] The end of training job file will not be written for jobs running under SageMaker.\u001b[0m\n",
      "\u001b[34m2020-07-19 13:36:57,495 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2020-07-19 13:37:10 Uploading - Uploading generated training model\n",
      "2020-07-19 13:37:10 Completed - Training job completed\n",
      "Training seconds: 1223\n",
      "Billable seconds: 1223\n",
      "CPU times: user 2.99 s, sys: 160 ms, total: 3.15 s\n",
      "Wall time: 24min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Instantiate a pytorch estimator using default hyperparameters.\n",
    "pyt_estimator_initial = PyTorch(entry_point='train.py',\n",
    "                                source_dir='source',\n",
    "                                role=role,\n",
    "                                train_instance_count=1,\n",
    "                                train_instance_type=train_instance_type,\n",
    "                                output_path=pyt_output_path,\n",
    "                                sagemaker_session=sagemaker_session,\n",
    "                                framework_version='1.4',\n",
    "                                enable_sagemaker_metrics=True,\n",
    "                                hyperparameters={\n",
    "                                    'input-dim': test_X.shape[1],\n",
    "                                    'scalers-config': scalers_config_location\n",
    "                                })\n",
    "\n",
    "pyt_estimator_initial.fit({'train': s3_input_train, 'validation': s3_input_val})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n",
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..........................................!\n",
      "download: s3://sagemaker-eu-west-1-019518462631/capstone/nn_250000/test-2.csv.out to ../data/250000/test-2.csv.out\n",
      "CPU times: user 289 ms, sys: 50.1 ms, total: 339 ms\n",
      "Wall time: 3min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Create transformer object\n",
    "transformer = pyt_estimator_initial.transformer(instance_count=1, instance_type=deploy_instance_type, output_path=pyt_output_path)\n",
    "\n",
    "# Perform batch transfor on test set\n",
    "transformer.transform(test_path, content_type='text/csv', split_type='Line', wait=True)\n",
    "\n",
    "# Download the output file\n",
    "if LOCAL_MODE:\n",
    "    output_file = transformer.output_path + '/' + transformer.latest_transform_job.job_name + '/' + test_file + '.out'\n",
    "else:\n",
    "    output_file = transformer.output_path + '/' + test_file + '.out'\n",
    "\n",
    "# Output file is downloaded to the notebook.\n",
    "!aws s3 cp $output_file $data_dir\n",
    "!sed -i -e 's/\\]\\[/, /g' $data_dir/test-2.csv.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.668853\n",
      "Recal:    0.727840\n",
      "ROC AUC:  0.665635\n"
     ]
    }
   ],
   "source": [
    "# And the predictions are processed and metrics calculated.\n",
    "basePath = os.path.abspath(data_dir)\n",
    "predictions_file = os.path.join(basePath, test_file+'.out')\n",
    "predictions = pd.read_json(predictions_file)\n",
    "predictions = [round(num) for num in predictions.squeeze().values]\n",
    "\n",
    "print(\"Accuracy: {:4f}\".format(accuracy_score(test_y, predictions)))\n",
    "print(\"Recal:    {:4f}\".format(recall_score(test_y, predictions)))\n",
    "print(\"ROC AUC:  {:4f}\".format(roc_auc_score(test_y, predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial model obtained an AUC of **0.665635**, which is actually slightly worsed than the tuned XGBoost model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the NN model using the defaults, let's use a good set of parameters determined using a small data set (50000 items).\n",
    "\n",
    "* Batch size: 300\n",
    "* Size of hidden layers: 300\n",
    "* Epochs: 100 (if validation loss does not reduce for 10 epochs, training is stopped).\n",
    "* Dropout rate: 0.5\n",
    "* Learning rate: 0.001\n",
    "* Optimizer: ADAM\n",
    "* Weight initialization: Uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n",
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-19 13:43:12 Starting - Starting the training job...\n",
      "2020-07-19 13:43:14 Starting - Launching requested ML instances......\n",
      "2020-07-19 13:44:20 Starting - Preparing the instances for training.........\n",
      "2020-07-19 13:45:55 Downloading - Downloading input data...\n",
      "2020-07-19 13:46:15 Training - Downloading the training image...........\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2020-07-19 13:48:25,149 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2020-07-19 13:48:25,173 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\n",
      "2020-07-19 13:48:24 Training - Training image download completed. Training in progress.\u001b[34m2020-07-19 13:48:31,430 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-07-19 13:48:31,718 sagemaker-containers INFO     Module default_user_module_name does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2020-07-19 13:48:31,718 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2020-07-19 13:48:31,718 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2020-07-19 13:48:31,719 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python -m pip install . -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing /tmp/tmpbjw46uum/module_dir\u001b[0m\n",
      "\u001b[34mCollecting scikit-learn==0.22.1\n",
      "  Downloading scikit_learn-0.22.1-cp36-cp36m-manylinux1_x86_64.whl (7.0 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy>=0.17.0 in /opt/conda/lib/python3.6/site-packages (from scikit-learn==0.22.1->-r requirements.txt (line 1)) (1.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.11.0 in /opt/conda/lib/python3.6/site-packages (from scikit-learn==0.22.1->-r requirements.txt (line 1)) (1.16.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.6/site-packages (from scikit-learn==0.22.1->-r requirements.txt (line 1)) (0.15.1)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: default-user-module-name\n",
      "  Building wheel for default-user-module-name (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for default-user-module-name (setup.py): finished with status 'done'\n",
      "  Created wheel for default-user-module-name: filename=default_user_module_name-1.0.0-py2.py3-none-any.whl size=22628 sha256=7828e7d0e7c5e9387bf76733b3555f314298dc25a70223167b927c9eca0763c2\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-64hsf6yd/wheels/52/52/04/12a4fd1691301527c101b996070759dc0d2c8d0128c3e5ded3\u001b[0m\n",
      "\u001b[34mSuccessfully built default-user-module-name\u001b[0m\n",
      "\u001b[34mInstalling collected packages: scikit-learn, default-user-module-name\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.21.2\n",
      "    Uninstalling scikit-learn-0.21.2:\n",
      "      Successfully uninstalled scikit-learn-0.21.2\u001b[0m\n",
      "\u001b[34mSuccessfully installed default-user-module-name-1.0.0 scikit-learn-0.22.1\u001b[0m\n",
      "\u001b[34m2020-07-19 13:48:37,214 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"validation\": \"/opt/ml/input/data/validation\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"lr\": 0.001,\n",
      "        \"input-dim\": 17,\n",
      "        \"hidden-dim\": 300,\n",
      "        \"scalers-config\": \"s3://sagemaker-eu-west-1-019518462631/capstone/nn_250000/scalers.pkl\",\n",
      "        \"dropout-rate\": 0.5,\n",
      "        \"batch-size\": 300,\n",
      "        \"optimizer\": \"adam\",\n",
      "        \"weigth-init\": \"uniform\",\n",
      "        \"epochs\": 100\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"validation\": {\n",
      "            \"ContentType\": \"csv\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"ContentType\": \"csv\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2020-07-19-13-43-12-577\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-eu-west-1-019518462631/pytorch-training-2020-07-19-13-43-12-577/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch-size\":300,\"dropout-rate\":0.5,\"epochs\":100,\"hidden-dim\":300,\"input-dim\":17,\"lr\":0.001,\"optimizer\":\"adam\",\"scalers-config\":\"s3://sagemaker-eu-west-1-019518462631/capstone/nn_250000/scalers.pkl\",\"weigth-init\":\"uniform\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"ContentType\":\"csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"ContentType\":\"csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\",\"validation\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-eu-west-1-019518462631/pytorch-training-2020-07-19-13-43-12-577/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\",\"validation\":\"/opt/ml/input/data/validation\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch-size\":300,\"dropout-rate\":0.5,\"epochs\":100,\"hidden-dim\":300,\"input-dim\":17,\"lr\":0.001,\"optimizer\":\"adam\",\"scalers-config\":\"s3://sagemaker-eu-west-1-019518462631/capstone/nn_250000/scalers.pkl\",\"weigth-init\":\"uniform\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"ContentType\":\"csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"ContentType\":\"csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2020-07-19-13-43-12-577\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-eu-west-1-019518462631/pytorch-training-2020-07-19-13-43-12-577/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch-size\",\"300\",\"--dropout-rate\",\"0.5\",\"--epochs\",\"100\",\"--hidden-dim\",\"300\",\"--input-dim\",\"17\",\"--lr\",\"0.001\",\"--optimizer\",\"adam\",\"--scalers-config\",\"s3://sagemaker-eu-west-1-019518462631/capstone/nn_250000/scalers.pkl\",\"--weigth-init\",\"uniform\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VALIDATION=/opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_LR=0.001\u001b[0m\n",
      "\u001b[34mSM_HP_INPUT-DIM=17\u001b[0m\n",
      "\u001b[34mSM_HP_HIDDEN-DIM=300\u001b[0m\n",
      "\u001b[34mSM_HP_SCALERS-CONFIG=s3://sagemaker-eu-west-1-019518462631/capstone/nn_250000/scalers.pkl\u001b[0m\n",
      "\u001b[34mSM_HP_DROPOUT-RATE=0.5\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH-SIZE=300\u001b[0m\n",
      "\u001b[34mSM_HP_OPTIMIZER=adam\u001b[0m\n",
      "\u001b[34mSM_HP_WEIGTH-INIT=uniform\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=100\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python train.py --batch-size 300 --dropout-rate 0.5 --epochs 100 --hidden-dim 300 --input-dim 17 --lr 0.001 --optimizer adam --scalers-config s3://sagemaker-eu-west-1-019518462631/capstone/nn_250000/scalers.pkl --weigth-init uniform\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mModel will be trained on cuda\u001b[0m\n",
      "\u001b[34mGet data loader for file train.csv.\u001b[0m\n",
      "\u001b[34mGet data loader for file val.csv.\u001b[0m\n",
      "\u001b[34mMaking Net with input 17, hidden 300\u001b[0m\n",
      "\u001b[34mSetting initial weights on Linear(in_features=17, out_features=300, bias=True)\u001b[0m\n",
      "\u001b[34mY=0.24253562503633297\u001b[0m\n",
      "\u001b[34mSetting initial weights on Linear(in_features=300, out_features=300, bias=True)\u001b[0m\n",
      "\u001b[34mY=0.05773502691896257\u001b[0m\n",
      "\u001b[34mSetting initial weights on Linear(in_features=300, out_features=300, bias=True)\u001b[0m\n",
      "\u001b[34mY=0.05773502691896257\u001b[0m\n",
      "\u001b[34mSetting initial weights on Linear(in_features=300, out_features=300, bias=True)\u001b[0m\n",
      "\u001b[34mY=0.05773502691896257\u001b[0m\n",
      "\u001b[34mSetting initial weights on Linear(in_features=300, out_features=1, bias=True)\u001b[0m\n",
      "\u001b[34mY=0.05773502691896257\u001b[0m\n",
      "\u001b[34mShow model:\u001b[0m\n",
      "\u001b[34mNet(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=17, out_features=300, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=300, bias=True)\n",
      "    (5): Tanh()\n",
      "    (6): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): Dropout(p=0.5, inplace=False)\n",
      "    (8): Linear(in_features=300, out_features=300, bias=True)\n",
      "    (9): Tanh()\n",
      "    (10): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): Dropout(p=0.5, inplace=False)\n",
      "    (12): Linear(in_features=300, out_features=300, bias=True)\n",
      "    (13): Tanh()\n",
      "    (14): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (15): Dropout(p=0.5, inplace=False)\n",
      "    (16): Linear(in_features=300, out_features=1, bias=True)\n",
      "  )\n",
      "  (sig): Sigmoid()\u001b[0m\n",
      "\u001b[34m)\u001b[0m\n",
      "\u001b[34mSaving model params in /opt/ml/model/model_info.pth\u001b[0m\n",
      "\u001b[34mEpoch 1\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m[2020-07-19 13:48:43.937 algo-1:43 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2020-07-19 13:48:43.938 algo-1:43 INFO hook.py:191] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2020-07-19 13:48:43.938 algo-1:43 INFO hook.py:236] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2020-07-19 13:48:43.938 algo-1:43 INFO state_store.py:67] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2020-07-19 13:48:43.951 algo-1:43 INFO hook.py:376] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2020-07-19 13:48:43.951 algo-1:43 INFO hook.py:437] Hook is writing from the hook with pid: 43\n",
      "\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6922\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6803\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 2\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6815\n",
      "  Validation: model on cuda\n",
      "  Validation: Average loss: 0.6719\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 3\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6726\n",
      "  Validation: model on cuda\n",
      "  Validation: Average loss: 0.6533\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 4\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6608\n",
      "  Validation: model on cuda\n",
      "  Validation: Average loss: 0.6452\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 5\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6536\n",
      "  Validation: model on cuda\n",
      "  Validation: Average loss: 0.6369\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 6\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6500\n",
      "  Validation: model on cuda\n",
      "  Validation: Average loss: 0.6316\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 7\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6464\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6303\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 8\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6444\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6270\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 9\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6414\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6239\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 10\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6399\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6231\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 11\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6394\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6211\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 12\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6371\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6198\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 13\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6362\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6184\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 14\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6346\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6170\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 15\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6340\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6159\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 16\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6329\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6146\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 17\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6324\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6135\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 18\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6315\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6121\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 19\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6305\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6140\u001b[0m\n",
      "\u001b[34mEpoch 20\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6293\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6122\u001b[0m\n",
      "\u001b[34mEpoch 21\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6286\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6104\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 22\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6285\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6103\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 23\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6275\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6099\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 24\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6264\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6090\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 25\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6258\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6092\u001b[0m\n",
      "\u001b[34mEpoch 26\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6258\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6092\u001b[0m\n",
      "\u001b[34mEpoch 27\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6255\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6069\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 28\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6248\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6088\u001b[0m\n",
      "\u001b[34mEpoch 29\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6244\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6067\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 30\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6240\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6067\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 31\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6237\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6070\u001b[0m\n",
      "\u001b[34mEpoch 32\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6227\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6067\u001b[0m\n",
      "\u001b[34mEpoch 33\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6234\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6061\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 34\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6224\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6053\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 35\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6216\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6059\u001b[0m\n",
      "\u001b[34mEpoch 36\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6213\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6053\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 37\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6214\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6046\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 38\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6207\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6035\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 39\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6206\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6039\u001b[0m\n",
      "\u001b[34mEpoch 40\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6210\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6032\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 41\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6192\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6029\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 42\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6195\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6044\u001b[0m\n",
      "\u001b[34mEpoch 43\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6194\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6034\u001b[0m\n",
      "\u001b[34mEpoch 44\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6188\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6039\u001b[0m\n",
      "\u001b[34mEpoch 45\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6188\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6048\u001b[0m\n",
      "\u001b[34mEpoch 46\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6188\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6034\u001b[0m\n",
      "\u001b[34mEpoch 47\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6191\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6030\u001b[0m\n",
      "\u001b[34mEpoch 48\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6190\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6023\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 49\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6180\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6033\u001b[0m\n",
      "\u001b[34mEpoch 50\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6181\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6026\u001b[0m\n",
      "\u001b[34mEpoch 51\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6184\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6023\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 52\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6188\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6015\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 53\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6170\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6012\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 54\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6175\n",
      "  Validation: model on cuda\n",
      "  Validation: Average loss: 0.6021\u001b[0m\n",
      "\u001b[34mEpoch 55\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6175\n",
      "  Validation: model on cuda\n",
      "  Validation: Average loss: 0.6026\u001b[0m\n",
      "\u001b[34mEpoch 56\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6169\n",
      "  Validation: model on cuda\n",
      "  Validation: Average loss: 0.6005\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 57\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6164\n",
      "  Validation: model on cuda\n",
      "  Validation: Average loss: 0.6009\u001b[0m\n",
      "\u001b[34mEpoch 58\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6172\n",
      "  Validation: model on cuda\n",
      "  Validation: Average loss: 0.6013\u001b[0m\n",
      "\u001b[34mEpoch 59\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6163\n",
      "  Validation: model on cuda\n",
      "  Validation: Average loss: 0.6005\u001b[0m\n",
      "\u001b[34mEpoch 60\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6164\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6008\u001b[0m\n",
      "\u001b[34mEpoch 61\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6155\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6003\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 62\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6161\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5999\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 63\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6159\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6010\u001b[0m\n",
      "\u001b[34mEpoch 64\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6161\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6003\u001b[0m\n",
      "\u001b[34mEpoch 65\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6153\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6004\u001b[0m\n",
      "\u001b[34mEpoch 66\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6151\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6000\u001b[0m\n",
      "\u001b[34mEpoch 67\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6154\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6001\u001b[0m\n",
      "\u001b[34mEpoch 68\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6150\n",
      "  Validation: model on cuda\n",
      "  Validation: Average loss: 0.6003\u001b[0m\n",
      "\u001b[34mEpoch 69\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6146\n",
      "  Validation: model on cuda\n",
      "  Validation: Average loss: 0.6002\u001b[0m\n",
      "\u001b[34mEpoch 70\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6144\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5989\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 71\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6147\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5979\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 72\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6140\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5985\u001b[0m\n",
      "\u001b[34mEpoch 73\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6137\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5987\u001b[0m\n",
      "\u001b[34mEpoch 74\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6144\n",
      "  Validation: model on cuda\n",
      "  Validation: Average loss: 0.5992\u001b[0m\n",
      "\u001b[34mEpoch 75\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6135\n",
      "  Validation: model on cuda\n",
      "  Validation: Average loss: 0.5988\u001b[0m\n",
      "\u001b[34mEpoch 76\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6128\n",
      "  Validation: model on cuda\n",
      "  Validation: Average loss: 0.5982\u001b[0m\n",
      "\u001b[34mEpoch 77\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6134\n",
      "  Validation: model on cuda\n",
      "  Validation: Average loss: 0.5996\u001b[0m\n",
      "\u001b[34mEpoch 78\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6143\n",
      "  Validation: model on cuda\n",
      "  Validation: Average loss: 0.5995\u001b[0m\n",
      "\u001b[34mEpoch 79\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6137\n",
      "  Validation: model on cuda\n",
      "  Validation: Average loss: 0.5976\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 80\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6132\n",
      "  Validation: model on cuda\n",
      "  Validation: Average loss: 0.5984\u001b[0m\n",
      "\u001b[34mEpoch 81\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6133\n",
      "  Validation: model on cuda\n",
      "  Validation: Average loss: 0.5982\u001b[0m\n",
      "\u001b[34mEpoch 82\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6138\n",
      "  Validation: model on cuda\n",
      "  Validation: Average loss: 0.5981\u001b[0m\n",
      "\u001b[34mEpoch 83\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6135\n",
      "  Validation: model on cuda\n",
      "  Validation: Average loss: 0.5979\u001b[0m\n",
      "\u001b[34mEpoch 84\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6134\n",
      "  Validation: model on cuda\n",
      "  Validation: Average loss: 0.5975\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 85\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6130\n",
      "  Validation: model on cuda\n",
      "  Validation: Average loss: 0.5981\u001b[0m\n",
      "\u001b[34mEpoch 86\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6126\n",
      "  Validation: model on cuda\n",
      "  Validation: Average loss: 0.5978\u001b[0m\n",
      "\u001b[34mEpoch 87\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6130\n",
      "  Validation: model on cuda\n",
      "  Validation: Average loss: 0.5985\u001b[0m\n",
      "\u001b[34mEpoch 88\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6124\n",
      "  Validation: model on cuda\n",
      "  Validation: Average loss: 0.5980\u001b[0m\n",
      "\u001b[34mEpoch 89\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6128\n",
      "  Validation: model on cuda\n",
      "  Validation: Average loss: 0.5978\u001b[0m\n",
      "\u001b[34mEpoch 90\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6124\n",
      "  Validation: model on cuda\n",
      "  Validation: Average loss: 0.5980\u001b[0m\n",
      "\u001b[34mEpoch 91\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6123\n",
      "  Validation: model on cuda\n",
      "  Validation: Average loss: 0.5983\u001b[0m\n",
      "\u001b[34mEpoch 92\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6125\n",
      "  Validation: model on cuda\n",
      "  Validation: Average loss: 0.5974\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 93\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6122\n",
      "  Validation: model on cuda\n",
      "  Validation: Average loss: 0.5973\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 94\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6121\n",
      "  Validation: model on cuda\n",
      "  Validation: Average loss: 0.5976\u001b[0m\n",
      "\u001b[34mEpoch 95\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6127\n",
      "  Validation: model on cuda\n",
      "  Validation: Average loss: 0.5972\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 96\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6121\n",
      "  Validation: model on cuda\n",
      "  Validation: Average loss: 0.5979\u001b[0m\n",
      "\u001b[34mEpoch 97\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6119\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5980\u001b[0m\n",
      "\u001b[34mEpoch 98\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6125\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5980\u001b[0m\n",
      "\u001b[34mEpoch 99\n",
      "  Train: model on cuda\u001b[0m\n",
      "\n",
      "2020-07-19 13:58:50 Uploading - Uploading generated training model\u001b[34m  Train: Average loss: 0.6124\n",
      "  Validation: model on cuda\n",
      "  Validation: Average loss: 0.5985\u001b[0m\n",
      "\u001b[34mEpoch 100\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6119\n",
      "  Validation: model on cuda\n",
      "  Validation: Average loss: 0.5974\u001b[0m\n",
      "\u001b[34mBest training loss: 0.6119 in epoch 97\u001b[0m\n",
      "\u001b[34mBest validation loss: 0.5972 in epoch 95\u001b[0m\n",
      "\u001b[34m[2020-07-19 13:58:46.880 algo-1:43 INFO utils.py:25] The end of training job file will not be written for jobs running under SageMaker.\u001b[0m\n",
      "\u001b[34m2020-07-19 13:58:47,344 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2020-07-19 13:58:57 Completed - Training job completed\n",
      "Training seconds: 782\n",
      "Billable seconds: 782\n",
      "CPU times: user 2.11 s, sys: 117 ms, total: 2.22 s\n",
      "Wall time: 16min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "scalers_config_location = '{}/scalers.pkl'.format(pyt_output_path)\n",
    "\n",
    "# Instantiate a pytorch estimator using default hyperparameters.\n",
    "pyt_estimator = PyTorch(entry_point='train.py',\n",
    "                        source_dir='source',\n",
    "                        role=role,\n",
    "                        train_instance_count=1,\n",
    "                        train_instance_type=train_instance_type,\n",
    "                        output_path=pyt_output_path,\n",
    "                        sagemaker_session=sagemaker_session,\n",
    "                        framework_version='1.4',\n",
    "                        enable_sagemaker_metrics=True,\n",
    "                        hyperparameters={\n",
    "                            'input-dim': test_X.shape[1],\n",
    "                            'scalers-config': scalers_config_location,\n",
    "                            'batch-size': 300,\n",
    "                            'hidden-dim': 300,\n",
    "                            'epochs': 100,\n",
    "                            'dropout-rate': 0.5,\n",
    "                            'lr': 0.001,\n",
    "                            'optimizer': 'adam',\n",
    "                            'weigth-init': 'uniform'\n",
    "                        })\n",
    "\n",
    "pyt_estimator.fit({'train': s3_input_train, 'validation': s3_input_val})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to evaluate the endpoint on test data\n",
    "# returns a variety of model metrics\n",
    "def evaluate(predictor, test_features, test_labels, verbose=True):\n",
    "    \"\"\"\n",
    "    Evaluate a model on a test set given the prediction endpoint.  \n",
    "    Return binary classification metrics.\n",
    "    :param predictor: A prediction endpoint\n",
    "    :param test_features: Test features\n",
    "    :param test_labels: Class labels for test data\n",
    "    :param verbose: If True, prints a table of all performance metrics\n",
    "    :return: A dictionary of performance metrics.\n",
    "    \"\"\"\n",
    "    \n",
    "    # rounding and squeezing array\n",
    "    test_preds = np.squeeze(np.round(predictor.predict(test_features)))\n",
    "    \n",
    "    # calculate true positives, false positives, true negatives, false negatives\n",
    "    tp = np.logical_and(test_labels, test_preds).sum()\n",
    "    fp = np.logical_and(1-test_labels, test_preds).sum()\n",
    "    tn = np.logical_and(1-test_labels, 1-test_preds).sum()\n",
    "    fn = np.logical_and(test_labels, 1-test_preds).sum()\n",
    "    \n",
    "    # calculate binary classification metrics\n",
    "    recall = tp / (tp + fn)\n",
    "    precision = tp / (tp + fp)\n",
    "    accuracy = (tp + tn) / (tp + fp + tn + fn)\n",
    "    auc = roc_auc_score(test_labels, test_preds)\n",
    "    \n",
    "    # print metrics\n",
    "    if verbose:\n",
    "        print(pd.crosstab(test_labels, test_preds, rownames=['actuals'], colnames=['predictions']))\n",
    "        print(\"\\n{:<11} {:.4f}\".format('Recall:', recall))\n",
    "        print(\"{:<11} {:.4f}\".format('Precision:', precision))\n",
    "        print(\"{:<11} {:.4f}\".format('Accuracy:', accuracy))\n",
    "        print(\"{:<11} {:.4f}\".format('AUC:', auc))\n",
    "        print()\n",
    "                \n",
    "    return {'TP': tp, 'FP': fp, 'FN': fn, 'TN': tn, \n",
    "            'Precision': precision, 'Recall': recall, 'Accuracy': accuracy, 'AUC': auc}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n",
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!CPU times: user 446 ms, sys: 35.8 ms, total: 482 ms\n",
      "Wall time: 7min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "\n",
    "# Create a model from the trained estimator data and point to the prediction script\n",
    "model = PyTorchModel(model_data=pyt_estimator.model_data,\n",
    "                     entry_point='predict.py',\n",
    "                     source_dir='source',\n",
    "                     role=role,\n",
    "                     framework_version='1.4')\n",
    "\n",
    "predictor = model.deploy(initial_instance_count=1, instance_type=deploy_instance_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions    0.0    1.0\n",
      "actuals                  \n",
      "0            10915   6865\n",
      "1             5205  14515\n",
      "\n",
      "Recall:     0.7361\n",
      "Precision:  0.6789\n",
      "Accuracy:   0.6781\n",
      "AUC:        0.6750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metrics = evaluate(predictor, test_X, test_y, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model achieves an AUC of **0.675**, which is on par with a tuned XGBoost model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()\n",
    "model.delete_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning Hyperparameters of PyTorch model\n",
    "\n",
    "Let's create a new PyTorch model. The previously trained model already improved upon the initial model with default values.\n",
    "Earlier training on smaller data set already showed that the ADAM optimizer and a uniform weight initialization function provided better results than the alternatives (e.g. SGD). So these hyperparameters will be defined static.\n",
    "Epochs should also not be checked anymore. The training script will automatically save the model with the lowest validation loss and will stop automatically if validation loss does not reduce over 10 epochs.\n",
    "\n",
    "For other hyperparameters ranges are defined in relation to the values used in previous training. For hyperparameter tuning 20 different combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\n",
    "\n",
    "# Instantiate a pytorch estimator\n",
    "pyt_hyper_estimator = PyTorch(entry_point='train.py',\n",
    "                              source_dir='source',\n",
    "                              role=role,\n",
    "                              train_instance_count=1,\n",
    "                              train_instance_type=train_instance_type,\n",
    "                              output_path=pyt_output_path,\n",
    "                              sagemaker_session=sagemaker_session,\n",
    "                              framework_version='1.4',\n",
    "                              hyperparameters={\n",
    "                                  'input-dim': test_X.shape[1],\n",
    "                                  'scalers-config': scalers_config_location,\n",
    "                                  'epochs': 100,\n",
    "                                  'optimizer': 'adam',\n",
    "                                  'weigth-init': 'uniform',\n",
    "                                  'batch-size': 300,\n",
    "                                  'hidden-dim': 300,\n",
    "                                  'dropout-rate': 0.5,\n",
    "                                  'lr': 0.001})\n",
    "\n",
    "# Define hyperparameter ranges\n",
    "hyperparameter_ranges = {\n",
    "    'batch-size': CategoricalParameter([300, 512, 768, 1024]),\n",
    "    'hidden-dim': CategoricalParameter([test_X.shape[1]*30, test_X.shape[1]*35, test_X.shape[1]*40, test_X.shape[1]*45, test_X.shape[1]*50]),\n",
    "    'dropout-rate': CategoricalParameter([0.3, 0.4, 0.5, 0.6, 0.7]),\n",
    "    'lr': CategoricalParameter([0.0001, 0.0003, 0.0005, 0.001]),\n",
    "}\n",
    "\n",
    "# Define objective metric\n",
    "objective_metric_name = 'average validation loss'\n",
    "objective_type = 'Minimize'\n",
    "metric_definitions = [{'Name': 'average validation loss',\n",
    "                       'Regex': 'Validation: Average loss: ([0-9\\\\.]+)'}]\n",
    "\n",
    "# Create tuner\n",
    "pyt_tuner = HyperparameterTuner(pyt_hyper_estimator,\n",
    "                                objective_metric_name,\n",
    "                                hyperparameter_ranges,\n",
    "                                metric_definitions,\n",
    "                                max_jobs=30,\n",
    "                                max_parallel_jobs=4,\n",
    "                                objective_type=objective_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n",
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n",
      "WARNING:sagemaker:'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JobStatus: {'Completed': 0, 'InProgress': 0, 'RetryableError': 0, 'NonRetryableError': 0, 'Stopped': 0}\n",
      "ObjectiveStatus: {'Succeeded': 0, 'Pending': 0, 'Failed': 0}\n",
      "JobStatus: {'Completed': 0, 'InProgress': 4, 'RetryableError': 0, 'NonRetryableError': 0, 'Stopped': 0}\n",
      "ObjectiveStatus: {'Succeeded': 0, 'Pending': 4, 'Failed': 0}\n",
      "JobStatus: {'Completed': 4, 'InProgress': 4, 'RetryableError': 0, 'NonRetryableError': 0, 'Stopped': 0}\n",
      "ObjectiveStatus: {'Succeeded': 4, 'Pending': 4, 'Failed': 0}\n",
      "JobStatus: {'Completed': 8, 'InProgress': 4, 'RetryableError': 0, 'NonRetryableError': 0, 'Stopped': 0}\n",
      "ObjectiveStatus: {'Succeeded': 8, 'Pending': 4, 'Failed': 0}\n",
      "JobStatus: {'Completed': 8, 'InProgress': 4, 'RetryableError': 0, 'NonRetryableError': 0, 'Stopped': 0}\n",
      "ObjectiveStatus: {'Succeeded': 8, 'Pending': 4, 'Failed': 0}\n",
      "JobStatus: {'Completed': 12, 'InProgress': 4, 'RetryableError': 0, 'NonRetryableError': 0, 'Stopped': 0}\n",
      "ObjectiveStatus: {'Succeeded': 12, 'Pending': 4, 'Failed': 0}\n",
      "JobStatus: {'Completed': 16, 'InProgress': 4, 'RetryableError': 0, 'NonRetryableError': 0, 'Stopped': 0}\n",
      "ObjectiveStatus: {'Succeeded': 16, 'Pending': 4, 'Failed': 0}\n",
      "JobStatus: {'Completed': 18, 'InProgress': 4, 'RetryableError': 0, 'NonRetryableError': 0, 'Stopped': 0}\n",
      "ObjectiveStatus: {'Succeeded': 18, 'Pending': 4, 'Failed': 0}\n",
      "JobStatus: {'Completed': 21, 'InProgress': 4, 'RetryableError': 0, 'NonRetryableError': 0, 'Stopped': 0}\n",
      "ObjectiveStatus: {'Succeeded': 21, 'Pending': 4, 'Failed': 0}\n",
      "JobStatus: {'Completed': 24, 'InProgress': 4, 'RetryableError': 0, 'NonRetryableError': 0, 'Stopped': 0}\n",
      "ObjectiveStatus: {'Succeeded': 24, 'Pending': 4, 'Failed': 0}\n",
      "JobStatus: {'Completed': 27, 'InProgress': 3, 'RetryableError': 0, 'NonRetryableError': 0, 'Stopped': 0}\n",
      "ObjectiveStatus: {'Succeeded': 27, 'Pending': 3, 'Failed': 0}\n",
      "JobStatus: {'Completed': 29, 'InProgress': 1, 'RetryableError': 0, 'NonRetryableError': 0, 'Stopped': 0}\n",
      "ObjectiveStatus: {'Succeeded': 27, 'Pending': 1, 'Failed': 0}\n",
      "CPU times: user 436 ms, sys: 67.5 ms, total: 503 ms\n",
      "Wall time: 2h 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import time\n",
    "\n",
    "pyt_tuner.fit({'train': s3_input_train, 'validation': s3_input_val})\n",
    "\n",
    "pyt_tuning_job_info = sagemaker_session.sagemaker_client.describe_hyper_parameter_tuning_job(HyperParameterTuningJobName=pyt_tuner.latest_tuning_job.job_name)\n",
    "\n",
    "while pyt_tuning_job_info['HyperParameterTuningJobStatus'] == 'InProgress':\n",
    "    print(\"JobStatus: {}\".format(pyt_tuning_job_info['TrainingJobStatusCounters']))\n",
    "    print(\"ObjectiveStatus: {}\".format(pyt_tuning_job_info['ObjectiveStatusCounters']))\n",
    "    time.sleep(10 * 60)   \n",
    "    pyt_tuning_job_info = sagemaker_session.sagemaker_client.describe_hyper_parameter_tuning_job(HyperParameterTuningJobName=pyt_tuner.latest_tuning_job.job_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model\n",
    "\n",
    "Before training part of the data set was put aside for final evaluation of the model using a batch transform job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-19 16:33:26 Starting - Preparing the instances for training\n",
      "2020-07-19 16:33:26 Downloading - Downloading input data\n",
      "2020-07-19 16:33:26 Training - Training image download completed. Training in progress.\n",
      "2020-07-19 16:33:26 Uploading - Uploading generated training model\n",
      "2020-07-19 16:33:26 Completed - Training job completed\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2020-07-19 16:24:20,723 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2020-07-19 16:24:20,725 sagemaker-containers INFO     Failed to parse hyperparameter _tuning_objective_metric value average validation loss to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2020-07-19 16:24:20,748 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2020-07-19 16:24:20,753 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-07-19 16:24:21,068 sagemaker-containers INFO     Module default_user_module_name does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2020-07-19 16:24:21,069 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2020-07-19 16:24:21,069 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2020-07-19 16:24:21,069 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python -m pip install . -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing /tmp/tmp8spvvglc/module_dir\u001b[0m\n",
      "\u001b[34mCollecting scikit-learn==0.22.1\n",
      "  Downloading scikit_learn-0.22.1-cp36-cp36m-manylinux1_x86_64.whl (7.0 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy>=0.17.0 in /opt/conda/lib/python3.6/site-packages (from scikit-learn==0.22.1->-r requirements.txt (line 1)) (1.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.6/site-packages (from scikit-learn==0.22.1->-r requirements.txt (line 1)) (0.15.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.11.0 in /opt/conda/lib/python3.6/site-packages (from scikit-learn==0.22.1->-r requirements.txt (line 1)) (1.16.4)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: default-user-module-name\n",
      "  Building wheel for default-user-module-name (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for default-user-module-name (setup.py): finished with status 'done'\n",
      "  Created wheel for default-user-module-name: filename=default_user_module_name-1.0.0-py2.py3-none-any.whl size=22628 sha256=0c091e2e21f723bd8983ccb5461f679224b89659fbebe00e6f8389d2f9207592\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-2_9ne9ag/wheels/c7/52/78/093c90a7aadad2577ac3bf5656bdbcd2a05024da03463b4692\u001b[0m\n",
      "\u001b[34mSuccessfully built default-user-module-name\u001b[0m\n",
      "\u001b[34mInstalling collected packages: scikit-learn, default-user-module-name\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.21.2\n",
      "    Uninstalling scikit-learn-0.21.2:\n",
      "      Successfully uninstalled scikit-learn-0.21.2\u001b[0m\n",
      "\u001b[34mSuccessfully installed default-user-module-name-1.0.0 scikit-learn-0.22.1\u001b[0m\n",
      "\u001b[34m2020-07-19 16:24:26,220 sagemaker-containers INFO     Failed to parse hyperparameter _tuning_objective_metric value average validation loss to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2020-07-19 16:24:26,232 sagemaker-containers INFO     Failed to parse hyperparameter _tuning_objective_metric value average validation loss to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2020-07-19 16:24:26,257 sagemaker-containers INFO     Failed to parse hyperparameter _tuning_objective_metric value average validation loss to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2020-07-19 16:24:26,281 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_estimator_module\": \"sagemaker.pytorch.estimator\",\n",
      "        \"sagemaker_estimator_class_name\": \"PyTorch\"\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"validation\": \"/opt/ml/input/data/validation\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"lr\": \"0.001\",\n",
      "        \"input-dim\": 17,\n",
      "        \"hidden-dim\": \"680\",\n",
      "        \"scalers-config\": \"s3://sagemaker-eu-west-1-019518462631/capstone/nn_250000/scalers.pkl\",\n",
      "        \"dropout-rate\": \"0.3\",\n",
      "        \"batch-size\": \"300\",\n",
      "        \"optimizer\": \"adam\",\n",
      "        \"weigth-init\": \"uniform\",\n",
      "        \"epochs\": 100\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"validation\": {\n",
      "            \"ContentType\": \"csv\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"ContentType\": \"csv\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-200719-1537-013-0d88927a\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-eu-west-1-019518462631/pytorch-training-2020-07-19-15-37-50-394/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch-size\":\"300\",\"dropout-rate\":\"0.3\",\"epochs\":100,\"hidden-dim\":\"680\",\"input-dim\":17,\"lr\":\"0.001\",\"optimizer\":\"adam\",\"scalers-config\":\"s3://sagemaker-eu-west-1-019518462631/capstone/nn_250000/scalers.pkl\",\"weigth-init\":\"uniform\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_estimator_class_name\":\"PyTorch\",\"sagemaker_estimator_module\":\"sagemaker.pytorch.estimator\"}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"ContentType\":\"csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"ContentType\":\"csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\",\"validation\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-eu-west-1-019518462631/pytorch-training-2020-07-19-15-37-50-394/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_estimator_class_name\":\"PyTorch\",\"sagemaker_estimator_module\":\"sagemaker.pytorch.estimator\"},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\",\"validation\":\"/opt/ml/input/data/validation\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch-size\":\"300\",\"dropout-rate\":\"0.3\",\"epochs\":100,\"hidden-dim\":\"680\",\"input-dim\":17,\"lr\":\"0.001\",\"optimizer\":\"adam\",\"scalers-config\":\"s3://sagemaker-eu-west-1-019518462631/capstone/nn_250000/scalers.pkl\",\"weigth-init\":\"uniform\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"ContentType\":\"csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"ContentType\":\"csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-200719-1537-013-0d88927a\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-eu-west-1-019518462631/pytorch-training-2020-07-19-15-37-50-394/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch-size\",\"300\",\"--dropout-rate\",\"0.3\",\"--epochs\",\"100\",\"--hidden-dim\",\"680\",\"--input-dim\",\"17\",\"--lr\",\"0.001\",\"--optimizer\",\"adam\",\"--scalers-config\",\"s3://sagemaker-eu-west-1-019518462631/capstone/nn_250000/scalers.pkl\",\"--weigth-init\",\"uniform\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VALIDATION=/opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_LR=0.001\u001b[0m\n",
      "\u001b[34mSM_HP_INPUT-DIM=17\u001b[0m\n",
      "\u001b[34mSM_HP_HIDDEN-DIM=680\u001b[0m\n",
      "\u001b[34mSM_HP_SCALERS-CONFIG=s3://sagemaker-eu-west-1-019518462631/capstone/nn_250000/scalers.pkl\u001b[0m\n",
      "\u001b[34mSM_HP_DROPOUT-RATE=0.3\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH-SIZE=300\u001b[0m\n",
      "\u001b[34mSM_HP_OPTIMIZER=adam\u001b[0m\n",
      "\u001b[34mSM_HP_WEIGTH-INIT=uniform\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=100\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python train.py --batch-size 300 --dropout-rate 0.3 --epochs 100 --hidden-dim 680 --input-dim 17 --lr 0.001 --optimizer adam --scalers-config s3://sagemaker-eu-west-1-019518462631/capstone/nn_250000/scalers.pkl --weigth-init uniform\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mModel will be trained on cuda\u001b[0m\n",
      "\u001b[34mGet data loader for file train.csv.\u001b[0m\n",
      "\u001b[34mGet data loader for file val.csv.\u001b[0m\n",
      "\u001b[34mMaking Net with input 17, hidden 680\u001b[0m\n",
      "\u001b[34mSetting initial weights on Linear(in_features=17, out_features=680, bias=True)\u001b[0m\n",
      "\u001b[34mY=0.24253562503633297\u001b[0m\n",
      "\u001b[34mSetting initial weights on Linear(in_features=680, out_features=680, bias=True)\u001b[0m\n",
      "\u001b[34mY=0.03834824944236852\u001b[0m\n",
      "\u001b[34mSetting initial weights on Linear(in_features=680, out_features=680, bias=True)\u001b[0m\n",
      "\u001b[34mY=0.03834824944236852\u001b[0m\n",
      "\u001b[34mSetting initial weights on Linear(in_features=680, out_features=680, bias=True)\u001b[0m\n",
      "\u001b[34mY=0.03834824944236852\u001b[0m\n",
      "\u001b[34mSetting initial weights on Linear(in_features=680, out_features=1, bias=True)\u001b[0m\n",
      "\u001b[34mY=0.03834824944236852\u001b[0m\n",
      "\u001b[34mShow model:\u001b[0m\n",
      "\u001b[34mNet(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=17, out_features=680, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): BatchNorm1d(680, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Dropout(p=0.3, inplace=False)\n",
      "    (4): Linear(in_features=680, out_features=680, bias=True)\n",
      "    (5): Tanh()\n",
      "    (6): BatchNorm1d(680, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): Dropout(p=0.3, inplace=False)\n",
      "    (8): Linear(in_features=680, out_features=680, bias=True)\n",
      "    (9): Tanh()\n",
      "    (10): BatchNorm1d(680, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): Dropout(p=0.3, inplace=False)\n",
      "    (12): Linear(in_features=680, out_features=680, bias=True)\n",
      "    (13): Tanh()\n",
      "    (14): BatchNorm1d(680, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (15): Dropout(p=0.3, inplace=False)\n",
      "    (16): Linear(in_features=680, out_features=1, bias=True)\n",
      "  )\n",
      "  (sig): Sigmoid()\u001b[0m\n",
      "\u001b[34m)\u001b[0m\n",
      "\u001b[34mSaving model params in /opt/ml/model/model_info.pth\u001b[0m\n",
      "\u001b[34mEpoch 1\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6816\n",
      "  Validation: model on cuda\n",
      "  Validation: Average loss: 0.6549\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 2\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6474\n",
      "  Validation: model on cuda\n",
      "  Validation: Average loss: 0.6329\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 3\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6362\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6217\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 4\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6291\n",
      "  Validation: model on cuda\n",
      "  Validation: Average loss: 0.6214\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 5\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6241\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6169\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 6\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6209\n",
      "  Validation: model on cuda\n",
      "  Validation: Average loss: 0.6155\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 7\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6177\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6115\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 8\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6158\n",
      "  Validation: model on cuda\n",
      "  Validation: Average loss: 0.6103\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 9\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6130\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6074\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 10\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6110\n",
      "  Validation: model on cuda\n",
      "  Validation: Average loss: 0.6057\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 11\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6090\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6070\u001b[0m\n",
      "\u001b[34mEpoch 12\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6077\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6017\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 13\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6055\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6016\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 14\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6053\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5986\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 15\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6033\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.6003\u001b[0m\n",
      "\u001b[34mEpoch 16\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6019\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5989\u001b[0m\n",
      "\u001b[34mEpoch 17\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.6006\n",
      "  Validation: model on cuda\n",
      "  Validation: Average loss: 0.5981\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 18\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5995\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5964\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 19\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5992\n",
      "  Validation: model on cuda\n",
      "  Validation: Average loss: 0.5970\u001b[0m\n",
      "\u001b[34mEpoch 20\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5972\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5960\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 21\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5964\n",
      "  Validation: model on cuda\n",
      "  Validation: Average loss: 0.5961\u001b[0m\n",
      "\u001b[34mEpoch 22\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5958\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5947\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 23\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5951\n",
      "  Validation: model on cuda\n",
      "  Validation: Average loss: 0.5949\u001b[0m\n",
      "\u001b[34mEpoch 24\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5946\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5974\u001b[0m\n",
      "\u001b[34mEpoch 25\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5945\n",
      "  Validation: model on cuda\n",
      "  Validation: Average loss: 0.5962\u001b[0m\n",
      "\u001b[34mEpoch 26\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5922\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5929\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 27\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5917\n",
      "  Validation: model on cuda\n",
      "  Validation: Average loss: 0.5939\u001b[0m\n",
      "\u001b[34mEpoch 28\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5907\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5942\u001b[0m\n",
      "\u001b[34mEpoch 29\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5909\n",
      "  Validation: model on cuda\n",
      "  Validation: Average loss: 0.5928\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 30\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5907\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5926\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 31\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5897\n",
      "  Validation: model on cuda\n",
      "  Validation: Average loss: 0.5921\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 32\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5886\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5927\u001b[0m\n",
      "\u001b[34mEpoch 33\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5891\n",
      "  Validation: model on cuda\n",
      "  Validation: Average loss: 0.5916\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 34\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5886\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5935\u001b[0m\n",
      "\u001b[34mEpoch 35\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5874\n",
      "  Validation: model on cuda\n",
      "  Validation: Average loss: 0.5942\u001b[0m\n",
      "\u001b[34mEpoch 36\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5880\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5930\u001b[0m\n",
      "\u001b[34mEpoch 37\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5873\n",
      "  Validation: model on cuda\n",
      "  Validation: Average loss: 0.5918\u001b[0m\n",
      "\u001b[34mEpoch 38\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5860\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5930\u001b[0m\n",
      "\u001b[34mEpoch 39\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5858\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5927\u001b[0m\n",
      "\u001b[34mEpoch 40\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5854\n",
      "  Validation: model on cuda\n",
      "  Validation: Average loss: 0.5920\u001b[0m\n",
      "\u001b[34mEpoch 41\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5857\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5923\u001b[0m\n",
      "\u001b[34mEpoch 42\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5844\n",
      "  Validation: model on cuda\n",
      "  Validation: Average loss: 0.5906\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 43\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5843\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5903\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 44\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5847\n",
      "  Validation: model on cuda\n",
      "  Validation: Average loss: 0.5903\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 45\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5837\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5899\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 46\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5839\n",
      "  Validation: model on cuda\n",
      "  Validation: Average loss: 0.5906\u001b[0m\n",
      "\u001b[34mEpoch 47\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5824\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5907\u001b[0m\n",
      "\u001b[34mEpoch 48\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5816\n",
      "  Validation: model on cuda\n",
      "  Validation: Average loss: 0.5908\u001b[0m\n",
      "\u001b[34mEpoch 49\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5821\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5912\u001b[0m\n",
      "\u001b[34mEpoch 50\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5818\n",
      "  Validation: model on cuda\n",
      "  Validation: Average loss: 0.5924\u001b[0m\n",
      "\u001b[34mEpoch 51\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5811\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5894\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 52\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5822\n",
      "  Validation: model on cuda\n",
      "  Validation: Average loss: 0.5896\u001b[0m\n",
      "\u001b[34mEpoch 53\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5808\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5899\u001b[0m\n",
      "\u001b[34mEpoch 54\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5805\n",
      "  Validation: model on cuda\n",
      "  Validation: Average loss: 0.5893\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 55\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5808\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5896\u001b[0m\n",
      "\u001b[34mEpoch 56\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5804\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5908\u001b[0m\n",
      "\u001b[34mEpoch 57\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5804\n",
      "  Validation: model on cuda\n",
      "  Validation: Average loss: 0.5898\u001b[0m\n",
      "\u001b[34mEpoch 58\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5794\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5897\u001b[0m\n",
      "\u001b[34mEpoch 59\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5786\n",
      "  Validation: model on cuda\n",
      "  Validation: Average loss: 0.5907\u001b[0m\n",
      "\u001b[34mEpoch 60\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5790\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5901\u001b[0m\n",
      "\u001b[34mEpoch 61\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5793\n",
      "  Validation: model on cuda\n",
      "  Validation: Average loss: 0.5897\u001b[0m\n",
      "\u001b[34mEpoch 62\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5783\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5889\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 63\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5774\n",
      "  Validation: model on cuda\n",
      "  Validation: Average loss: 0.5887\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 64\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5774\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5891\u001b[0m\n",
      "\u001b[34mEpoch 65\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5777\n",
      "  Validation: model on cuda\n",
      "  Validation: Average loss: 0.5892\u001b[0m\n",
      "\u001b[34mEpoch 66\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5769\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5895\u001b[0m\n",
      "\u001b[34mEpoch 67\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5763\n",
      "  Validation: model on cuda\n",
      "  Validation: Average loss: 0.5901\u001b[0m\n",
      "\u001b[34mEpoch 68\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5770\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5885\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 69\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5764\n",
      "  Validation: model on cuda\n",
      "  Validation: Average loss: 0.5885\u001b[0m\n",
      "\u001b[34mEpoch 70\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5769\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5872\n",
      "  Model performed better on validation...Saved.\u001b[0m\n",
      "\u001b[34mEpoch 71\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5764\n",
      "  Validation: model on cuda\n",
      "  Validation: Average loss: 0.5889\u001b[0m\n",
      "\u001b[34mEpoch 72\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5749\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5883\u001b[0m\n",
      "\u001b[34mEpoch 73\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5753\n",
      "  Validation: model on cuda\n",
      "  Validation: Average loss: 0.5882\u001b[0m\n",
      "\u001b[34mEpoch 74\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5750\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5896\u001b[0m\n",
      "\u001b[34mEpoch 75\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5752\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5880\u001b[0m\n",
      "\u001b[34mEpoch 76\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5749\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5892\u001b[0m\n",
      "\u001b[34mEpoch 77\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5739\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5891\u001b[0m\n",
      "\u001b[34mEpoch 78\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5747\n",
      "  Validation: model on cuda\n",
      "  Validation: Average loss: 0.5887\u001b[0m\n",
      "\u001b[34mEpoch 79\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5743\n",
      "  Validation: model on cuda\u001b[0m\n",
      "\u001b[34m  Validation: Average loss: 0.5900\u001b[0m\n",
      "\u001b[34mEpoch 80\n",
      "  Train: model on cuda\u001b[0m\n",
      "\u001b[34m  Train: Average loss: 0.5745\n",
      "  Validation: model on cuda\n",
      "  Validation: Average loss: 0.5881\n",
      "  Validation loss didn't improve over last 10 epochs. Stopping\u001b[0m\n",
      "\u001b[34mBest training loss: 0.5739 in epoch 77\u001b[0m\n",
      "\u001b[34mBest validation loss: 0.5872 in epoch 70\u001b[0m\n",
      "\u001b[34m2020-07-19 16:33:16,956 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "Training seconds: 686\n",
      "Billable seconds: 686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!CPU times: user 826 ms, sys: 38.9 ms, total: 865 ms\n",
      "Wall time: 7min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "\n",
    "# Create a model from the trained estimator data and point to the prediction script\n",
    "model = PyTorchModel(model_data=pyt_tuner.best_estimator().model_data,\n",
    "                     entry_point='predict.py',\n",
    "                     source_dir='source',\n",
    "                     role=role,\n",
    "                     framework_version='1.4')\n",
    "\n",
    "predictor = model.deploy(initial_instance_count=1, instance_type=deploy_instance_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions    0.0    1.0\n",
      "actuals                  \n",
      "0            11067   6713\n",
      "1             5062  14658\n",
      "\n",
      "Recall:     0.7433\n",
      "Precision:  0.6859\n",
      "Accuracy:   0.6860\n",
      "AUC:        0.6829\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metrics = evaluate(predictor, test_X, test_y, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
